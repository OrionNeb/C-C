---
lang: de  # Dokumentensprache: Deutsch

format:
  pdf:
    documentclass: book
    classoption: openany, oneside  # Kapitel auf jeder Seite starten (verhindert leere Seiten)
    mainfont: "Latin Modern Roman"
    fontsize: 11pt
    linestretch: 1.5  # Zeilenabstand auf 1,5 setzen (für bessere Lesbarkeit)
    code-fold: false  # Code-Chunks bleiben immer sichtbar (nicht einklappbar)
    toc: false
    toc-depth: 4
    number-sections: true
    unnumbered-sections: true
    geometry: "top=3cm, bottom=3cm, headheight=15pt, headsep=1.5cm, footskip=1.5cm"
    pdf-engine: lualatex
    listings: false
    highlight-style: tango  # Syntax-Highlighting für Code
    code-block-font-size: \tiny
    code-overflow: wrap
    include-in-header: 
      - setup/header.qmd
      - setup/glossar-eintraege.qmd
    include-before-body:
      - setup/titlepage.qmd
      - setup/gendererklaerung.qmd
      - setup/abstract.qmd
      - setup/verzeichnisse.qmd
      - setup/mainmatter.qmd
    include-after-body:
      - setup/glossar.qmd
      - setup/ehenwoertlicheErklaerung.qmd
  html:
    code-fold: true  # Code-Chunks in HTML können ausgeklappt werden
    toc: false
    toc-depth: 4
    number-sections: true
    self-contained: true  # Alle Ressourcen (Bilder, CSS, etc.) in einer Datei speichern
    other-formats: ["pdf", "docx", "pptx"]
  docx:
    toc: false
    number-sections: true
  pptx:
    reference-doc: setup/FHDW-Powerpoint-Vorlage_16zu9.pptx  # (Optional) Eigene Vorlage nutzen
bibliography: setup/references.yaml
citeproc: true
csl: setup/apa-with-abstract-de.csl # https://www.zotero.org/styles/apa-with-abstract
link-citations: true
link-bibliography: true
editor: source
execute:
  freeze: auto
  eval: true
  echo: true       # Code anzeigen
  results: markup  # Output als formatierten Text ausgeben
  warning: false   # Warnungen unterdrücken
  message: false   # Nachrichten (z. B. von Paketen) unterdrücken
  code-overflow: wrap  # Lange Zeilen umbrechen
---



```{r eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}
# Installiere und lade erforderliche Bibliotheken
if (!requireNamespace("reticulate", quietly = TRUE)) install.packages("reticulate")
if (!requireNamespace("rmarkdown", quietly = TRUE)) install.packages("rmarkdown")
if (!requireNamespace("shiny", quietly = TRUE)) install.packages("shiny")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("tidyr", quietly = TRUE)) install.packages("tidyr")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
if (!requireNamespace("forecast", quietly = TRUE)) install.packages("forecast")
if (!requireNamespace("ggpubr", quietly = TRUE)) install.packages("ggpubr")
# if (!requireNamespace("viridis", quietly = TRUE)) install.packages("viridis")




library(reticulate)
library(rmarkdown)
library(shiny)
library(dplyr)
library(tidyr)
library(tidyverse)
library(forecast)
library(ggpubr)
# library(viridis)
```

```{python, include=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Erforderliche Bibliotheken installieren (falls nicht vorhanden)
import subprocess
import sys

def install_and_import(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = __import__(package)

# Äquivalente Bibliotheken in Python installieren und importieren
libraries = ["pandas", "numpy", "matplotlib", "seaborn", "scipy"]

for lib in libraries:
    install_and_import(lib)

# Bibliotheken laden
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.seasonal import STL
from pulp import LpMaximize, LpProblem, LpVariable, value

```



<!-- ================= 1. Einleitung ================= -->

# Einleitung {#sec-einleitung}

## Problemstellung {#sec-problemstellung}

Unternehmen segmentieren Kunden oft je Datenquelle, etwa nur im Clickstream oder nur in Transaktionen. So entstehen Cluster mit begrenzter Übertragbarkeit. CRM benötigt jedoch stabile, interpretierbare Segmente, die Nutzungs- und Kaufverhalten zusammenführen und ohne personenbezogene Daten auskommen.


## Zielsetzung {#sec-zielsetzung}

Die Arbeit vereinheitlicht RetailRocket und Online Retail II zu einem Ereignisschema und beschreibt Kunden mit einem gemeinsamen Merkmalset aus RFM, Konversionsraten, Zeitmustern, Diversität, Zwischenkaufintervallen sowie Text-Mining auf Kategoriepfaden. Darauf aufbauend werden K-Means und HDBSCAN angewandt und verglichen. Qualität, Stabilität und Clustertendenz werden mit Silhouette-Index, Davies-Bouldin-Index, DBCV, Bootstrapping, Adjusted Rand Index und der Hopkins-Statistik bewertet.



## Vorgehensweise {#sec-vorgehensweise}

Die Untersuchung folgt CRISP-DM mit Datenverständnis und EDA, Vereinheitlichung, Merkmalserstellung, Modellierung, Evaluation und Profiling zu CRM-Segmenten. Leitend ist die Frage, wie robust und übertragbar Kundencluster aus RetailRocket und Online Retail II sind, die mit K-Means und HDBSCAN gebildet werden, gemessen mit Silhouette-Index, Davies-Bouldin-Index, DBCV und Adjusted Rand Index, und wie sie zu geschäftlich interpretierbaren Segmenten profiliert werden können.




<!-- ================= 2. Theoretische Grundlagen ================= -->

# Grundlagen {#sec-grundlagen}

CRISP-DM bildet den Prozessrahmen dieser Arbeit. Wirth und Hipp schreiben: „The life cycle of a data mining project is broken down in six phases“ (@wirthCRISPDMStandardProcess2000, S. 4). Die Phasen reichen von Business Understanding über Data Understanding, Data Preparation, Modeling und Evaluation bis zu Deployment; sie werden iterativ durchlaufen (@wirthCRISPDMStandardProcess2000, S. 4–7). Wirth und Hipp sprechen von „this highly iterative, creative process with many parallel activities“ und halten fest: „it is never the case that a phase is completely done before the subsequent phase starts“ (@wirthCRISPDMStandardProcess2000, S. 9). Dieser Rahmen stellt sicher, dass Ziele, Daten, Modelle und Bewertung systematisch aufeinander aufbauen und ist in [Anhang A: CRISP-DM](#sec-anhangA), [Abschnitt A1: Das CRISP-DM-Modell](#sec-anhangA1) grafisch dargestellt.

## Kundensegmentierung im CRM Kontext {#sec-kundensegmentierung}

Clustering wird im CRM als Verfahren des unüberwachten Lernens eingesetzt, das natürliche Gruppierungen erkennt und Kunden in intern kohäsive Gruppen einordnet. „Clustering techniques identify meaningful natural groupings of records and group customers into distinct segments with internal cohesion“ (@tsiptsisDataMiningTechniques2010, S. 39). Ziel ist die Bildung unterscheidbarer Kundentypologien, „so that they can be marketed more effectively“ (@tsiptsisDataMiningTechniques2010, S. 40). Der Nutzen einer Segmentierung misst sich daran, ob die resultierenden Typologien transparent, aussagekräftig und handlungsleitend sind. „The value of each solution depends on its ability to represent transparent, meaningful, and actionable customer typologies“ (@tsiptsisDataMiningTechniques2010, S. 129). Als visuelle Einordnung dient im [Anhang B: Kundensegmentierung](#sec-anhangB), Abschnitt [Anhang B.1: Einordnung der Kundensegmentierung unter Unsupervised Learning](#sec-anhangB1) die Lernarten-Übersicht, in der Kundensegmentierung unter Unsupervised Learning verortet ist.

## Zwei unabhängige Datensichten und deren Harmonisierung im Lichte von CRISP-DM {#sec-crisp-dm}

Business Understanding konkretisiert das Ziel auf robuste und übertragbare Segmente. Data Understanding führt zwei unabhängige, aber komplementäre Datensichten ein: eine verhaltensnahe Ereignissicht und eine wertbasierte Rechnungssicht. Erst die Kombination erlaubt zu prüfen, ob Clusterstrukturen auf gleich definierten Merkmalen in unterschiedlichen Datenwelten wiederkehren und damit eher domänenweit gültige Muster darstellen als datensatzspezifische Effekte (@wirthCRISPDMStandardProcess2000, S. 5; @tsiptsisDataMiningTechniques2010, S. 39–40).

In Data Preparation werden beide Quellen in ein gemeinsames Ereignisschema überführt, um Messäquivalenz herzustellen. Praktisch umfasst dies konsistente Felder wie customer_id, item_id, timestamp und event_type sowie bei Bedarf quantity, price, revenue und eine Kategoriendarstellung. Damit lassen sich zentrale Konstrukte wie Recency, Frequency, Interkaufintervalle, Diversität und Zeitmuster in beiden Datensichten identisch operationalisieren (@wirthCRISPDMStandardProcess2000, S. 5–6).

In Modeling werden komplementäre Verfahren auf denselben Kernmerkmalen je Datensicht kalibriert, etwa ein zentroidbasiertes und ein dichtebasiertes Verfahren, um unterschiedliche Strukturannahmen abzudecken (@wirthCRISPDMStandardProcess2000, S. 6). Die Evaluation kombiniert interne Qualitätsmaße, die zur jeweiligen Methode passen, mit Stabilitäts- und Übertragbarkeitstests zwischen den Datensichten, bevor Ergebnisse reproduzierbar dokumentiert werden (@wirthCRISPDMStandardProcess2000, S. 6–7, S. 9).

## Datenquellen und Datenschutz RetailRocket und Online Retail II {#sec-datenquellen}

Diese Studie nutzt zwei offen zugängliche, anonymisierte E-Commerce-Datensätze mit komplementärer Sicht. Der RetailRocket-Datensatz wurde vom Anbieter auf Kaggle veröffentlicht und enthält roh erfasste Shop-Ereignisse sowie artikelbezogene Eigenschaften und den Kategoriebaum. Zentrale Dateien sind events.csv mit den Ereignistypen view, addtocart und transaction, item_properties_part1.csv, item_properties_part2.csv sowie category_tree.csv. Identifikatoren für Nutzer und Artikel sind gehasht; personenbezogene Daten liegen nicht vor. Die Kaggle-Seite gilt als Primärreferenz des Datensatzes (@zykovRetailrocketRecommenderSystem2022). Online Retail II ist eine von UCI kuratierte Transaktionshistorie eines britischen Händlers mit Produktcodes, Mengen, Preisen, Zeitstempeln und einer Kundenkennung. Inhaltlich wird die UCI-Landingpage zitiert; der praktische Download erfolgte über den Kaggle-Mirror (@DatasetsUCIMachine2019; @miyabonOnlineRetailII2009). Für diese Arbeit genügt ein knapper Datenschutzhinweis, da ausschließlich anonymisierte Forschungsdaten verwendet werden.

## Verfahren und Qualitätsmaße im Überblick K Means, HDBSCAN und passende Indizes {#sec-verfahren-qualitaet}

K-Means dient in dieser Arbeit als zentroidbasierte Baseline. Die folgenden Zitate belegen genau die Punkte, die für unser Vorgehen entscheidend sind Iterationsprinzip und Zielgröße, Vorabwahl der Clusterzahl und Zentroidrepräsentation sowie Distanzmaß und Konvergenzkriterium. Erstens zum Iterationsprinzip und Ziel K-Means „starts with an initial cluster solution which is updated and adjusted until no further refinement is possible … Each iteration refines the solution by reducing the within-cluster variation“ (@tsiptsisDataMiningTechniques2010, S. 85). Zweitens zur Modellannahme über die Segmentzahl „The "K" in the algorithm’s name comes from the fact that users should specify in advance the number of k clusters to be formed. The "means" part of the name refers to the fact that each cluster is represented by the means of its records … the cluster central point or centroid“ (@tsiptsisDataMiningTechniques2010, S. 85). Drittens zum Ablauf und zur Konvergenz „K-means uses the Euclidean distance measure … The procedure starts by selecting k well-spaced initial records as cluster centers … This iterative procedure is repeated until it converges and the migration of records between clusters no longer refines the solution“ (@tsiptsisDataMiningTechniques2010, S. 86).
Die herangezogenen Quellen belegen präzise die methodischen Voraussetzungen dieser Studie: ein standardisierter numerischer Merkmalsraum mit euklidischer Distanz sowie die transparente Bestimmung der Clusterzahl auf Basis geeigneter Indizes. Zugleich verdeutlichen sie, warum K-Means klar interpretierbare Zentroidprofile erzeugt, die sich unmittelbar für das anschließende Segmentprofiling nutzen lassen.

HDBSCAN steht für eine hierarchische Weiterentwicklung dichtebasierter Clusterverfahren. Dichtebasierte Ansätze identifizieren Gruppen dort, wo Beobachtungen in Regionen hoher Punktdichte liegen, und behandeln spärlich besetzte Bereiche als Rauschen. Das zugrunde liegende Prinzip wird bei DBSCAN anhand der Parameter $\varepsilon$ und MinPts eingeführt; dadurch lassen sich nicht-konvexe Formen erkennen und Ausreißer explizit als noise ausweisen (@chakrabortyDataClassificationIncremental2022, S. 87–89). HDBSCAN knüpft an dieses Prinzip an, setzt jedoch auf eine hierarchische Betrachtung der Dichte, sodass Cluster über ein Spektrum von Dichteschwellen hinweg ermittelt und stabilere Gruppierungen extrahiert werden können. Damit entfällt die feste Vorgabe einer Clusterzahl und es entstehen Lösungen, die mit lokal unterschiedlichen Dichten und Rauschen umgehen können.

Der methodische Kontrast zu zentroidbasierten Verfahren ist dabei grundlegend. Während K-Means vor allem bei annähernd kugelförmigen Strukturen überzeugt, eignen sich hierarchische Verfahren für arbiträr geformte Cluster (@chakrabortyDataClassificationIncremental2022, S. 96). In dieser Arbeit wird HDBSCAN deshalb als dichtebasiertes Gegenstück zu K-Means eingesetzt; die konkrete Parametrisierung und der algorithmische Ablauf folgen im Methodenteil.

Abschließend zum Methodenvergleich ist der Umgang mit Ausreißern wichtig. In E-Commerce-Daten treten extreme Preise, Mengen oder Event-Bursts auf. K-Means reagiert darauf empfindlich, weil einzelne Extreme Zentren und Distanzen verschieben, während dichtebasierte Verfahren dünn besetzte Bereiche als Rauschen behandeln. Die zugrunde liegende Idee wird beim dichtebasierten Clustering mit $\varepsilon$ und MinPts erläutert, die Outlier-Analyse ordnet Begriffe und Vorgehen ein (@chakrabortyDataClassificationIncremental2022, S. 87–89). Darauf baut die regelbasierte Ausreißerbehandlung in Kapitel 3 auf.

## Qualitäts- und Stabilitätsmaße im Überblick {#sec-qualitaet}

Zur Bewertung der Clusterlösungen werden für jedes Verfahren passende Gütemaße verwendet. Für K-Means liegt der Schwerpunkt auf dem Silhouettenkoeffizienten. Dieses Maß erfasst, wie gut ein Objekt zu seinem eigenen Cluster passt, verglichen mit der Nähe zu anderen Clustern. Der Wert reicht von –1 bis +1, wobei höhere Werte auf klar abgegrenzte Clusterstrukturen hinweisen. Tsiptsis und Chorianopoulos nennen die Silhouette als ein zentrales Kriterium für die technische Evaluation von Clustern (@tsiptsisDataMiningTechniques2010, S. 209). Lai et al. betonen, dass der Silhouettenkoeffizient sowohl die Kohäsion innerhalb der Cluster als auch die Trennung zwischen Clustern berücksichtigt und damit ein objektives Maß für die Güte der Gruppierung liefert (@laiSilhouetteCoefficientbasedWeighting2025, S. 3063).

Für dichtebasierte Verfahren wie HDBSCAN eignet sich der DBCV-Index. Moulavi et al. betonen, dass „DBCV employs the concept of Hartigan’s model of density-contour trees to compute the least dense region inside a cluster and the most dense region between the clusters, which are used to measure the within and between-cluster density connectedness of clusters“ (@moulaviDensityBasedClusteringValidation2014, S. 839). Damit wird die Qualität einer Clustering-Lösung nicht mehr über Abstände zwischen Zentren gemessen, sondern über dichtebasierte Eigenschaften. Ein guter Cluster hat dabei eine höhere Mindestdichte im Inneren als die maximale Dichte, die ihn von anderen Clustern trennt. Zudem stellen die Autoren klar: „Unlike other relative validity indices, our method not only directly takes into account density and shape properties of clusters but also properly deals with noise objects, which are intrinsic to the definition of the density-based clustering“ (@moulaviDensityBasedClusteringValidation2014, S. 845). Das bedeutet, dass das Verfahren auch Ausreißer und Rauschen korrekt berücksichtigt, die bei dichtebasierten Algorithmen wie HDBSCAN typischerweise auftreten. So lässt sich die Qualität der Lösung auch dann zuverlässig beurteilen, wenn die Cluster keine einfachen, kugelförmigen Strukturen bilden.

Ergänzend zur reinen Gütebewertung wird die Stabilität geprüft. Hierzu werden wiederholt Stichproben aus den Daten gezogen, Modelle neu geschätzt und die Übereinstimmung der Clusterzuordnungen mit dem Adjusted Rand Index gemessen. Werte nahe bei 1 deuten auf eine stabile Lösung hin. Vor der eigentlichen Modellierung wird zudem mit der Hopkins-Statistik getestet, ob der Datensatz überhaupt eine ausgeprägte Clustertendenz enthält. Werte nahe 0,5 sprechen für Zufälligkeit, während Werte über 0,75 auf deutliche Clusterstrukturen hindeuten.

Damit deckt die Arbeit drei Dimensionen ab: die innere Qualität der Cluster, die Robustheit der Ergebnisse und die grundsätzliche Tendenz zur Bildung von Gruppen. Diese Kombination ermöglicht es, K-Means und HDBSCAN fair miteinander zu vergleichen.

## Übergang zum Praxisteil im Lichte von CRISP-DM {#sec-uebergang}

Die folgenden Schritte orientieren sich eng am CRISP-DM-Referenzmodell und werden iterativ durchlaufen. Wirth und Hipp betonen: „The sequence of the phases is not strict. The arrows indicate only the most important and frequent dependencies between phases, but in a particular project, it depends on the outcome of each phase which phase, or which particular task of a phase, has to be performed next.“ (@wirthCRISPDMStandardProcess2000, S. 4)  
Genau diese flexible und zyklische Struktur nutzen wir, um Datenaufbereitung, Merkmalsbildung und Modellwahl eng mit den Evaluationsbefunden zu koppeln.

Konkret bedeutet das für den Praxisteil: Zunächst vertiefen wir Data Understanding für beide Quellen und legen Datenqualität, Beobachtungsfenster und Clustertendenz fest. In Data Preparation harmonisieren wir RetailRocket und Online Retail II in ein gemeinsames Ereignisschema, sodass RFM-, Zeit- und Diversitätsmerkmale messäquivalent berechnet werden. In Modeling setzen wir zwei komplementäre Verfahren ein: K-Means als zentroidbasierte Baseline und HDBSCAN als hierarchisch dichtebasiertes Gegenstück. Die Evaluation folgt dem in Abschnitt [Qualitäts- und Stabilitätsmaße im Überblick](#sec-qualitaet) beschriebenen Set aus Silhouette, Calinski-Harabasz, Davies-Bouldin bzw. DBCV, ergänzt um Bootstrapping und ARI zur Stabilität sowie den Rauschanteil bei HDBSCAN.

Zur Orientierung, wie diese Schritte in dieser Arbeit konkret den CRISP-DM-Phasen zugeordnet sind, verweist [Anhang A: CRISP-DM](#sec-anhangA), [Abschnitt A.2: Das CRISP-DM-Modell angewendet auf diese Studienarbeit](#sec-anhangA2).

<!-- ================= 3. Praxisteil ================= -->

# Praxisteil

## Datenverständnis RetailRocket Ereignisschema und Eigenschaften {#sec-datenverstaendnisRR}

## Datenverständnis Online Retail II Transaktionsschema und Eigenschaften {#sec-datenverstaendnisOR}

## Vereinheitlichung beider Datensätze gemeinsames Ereignisschema {#sec-datenvereinheitlichung}

## Datenaufbereitung und Feature Engineering RFM Konversionsproxys Zeitmuster Diversität Interkaufintervalle {#sec-datenaufbereitung}

## Text Mining auf Kategoriepfaden TF IDF optional Item2Vec {#sec-textmining}

## Clustering Ansatz 1 K-Means Auswahl der Clusterzahl über Silhouette und Davies Bouldin {#sec-clustering1}

## Clustering Ansatz 2 HDBSCAN Parameterwahl und DBCV {#sec-clustering2}

## Stabilität und Robustheit Bootstrapping und Adjusted Rand Index {#sec-stabilitaet}

## Segmentprofiling KPI Profile und Top Kategorien je Cluster {#sec-segmentprofiling}

## Reproduzierbarkeit Notebook Struktur Hyperparameter Versionierung {#sec-reproduzierbarkeit}

<!-- ================= 4. Evaluation und Ergebnisse ================= -->

# Evaluation und Ergebnisse

## Clusterqualität je Datensatz RetailRocket und Online Retail II {#sec-clusterqualitaet}

## Vergleich der Lösungen K-Means gegenüber HDBSCAN {#sec-vergleich}

## Sensitivitätsanalyse Feature Varianten und Hyperparameter {#sec-sensitivitaetsanalyse}

## Grenzen Datenqualität Sparsity Cold Start {#sec-grenzen}

<!-- ================= 5. Fazit und Ausblick ================= -->

# Fazit und Ausblick



Was eine \gls{mfa} ist, wird im Glossar beschrieben. Auch glspl und glslink sind möglich.


## Beantwortung der Forschungsfrage
## Nutzen für CRM, Kampagnensteuerung und nächste Schritte
## Ausblick weitere Text Mining Merkmale und Uplift Experimente



<!-- ================= Literaturverzeichnis ================= -->

# Literaturverzeichnis {.unnumbered}

::: {#refs}
:::
\pagestyle{literature}
\markboth{Literaturverzeichnis}{Literaturverzeichnis}

\clearpage

<!-- ================= Anhangsverzeichnis-Header ================= -->

::: {#sec-anhang}
# Anhang {.unnumbered}
:::
\markboth{Anhang}{Anhang} 
\thispagestyle{empty}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0}



::: {#sec-anhangsverzeichnis}
## Anhangsverzeichnis {.unnumbered}
:::
\pagestyle{appendixTOC}
\noindent
<!-- ================= Anhangsverzeichnis ================= -->

**A** [CRISP-DM](#sec-anhangA) \dotfill  \pageref{sec-anhangA}

&nbsp;&nbsp;&nbsp;**A.1** [Das CRISP-DM-Modell](#sec-anhangA1) \dotfill  \pageref{sec-anhangA1}

&nbsp;&nbsp;&nbsp;**A.2** [Das CRISP-DM-Modell angewendet auf diese Studienarbeit](#sec-anhangA1) \dotfill  \pageref{sec-anhangA1}

**B** [Kundensegmentierung](#sec-anhangB) \dotfill  \pageref{sec-anhangB}

&nbsp;&nbsp;&nbsp;**B.1** [Einordnung der Kundensegmentierung unter Unsupervised Learning](#sec-anhangB1) \dotfill  \pageref{sec-anhangB1}

**C** [Erste Analyse der Daten](#sec-anhangC) \dotfill  \pageref{sec-anhangC}

&nbsp;&nbsp;&nbsp;**C.1** [Ergebnisse der ersten Analyse](#sec-anhangC1) \dotfill  \pageref{sec-anhangC1}
 





\clearpage

<!-- ================= Anhang ================= -->
\pagestyle{appendix} 

::: {#sec-anhangA}
# A: CRISP-DM {.unnumbered}
:::
\markboth{Anhang A: CRISP-DM}{}

::: {#sec-anhangA1}
## A.1: Das CRISP-DM-Modell {.unnumbered}
:::

Die @fig-crisp-dm zeigt das CRISP-DM-Referenzmodell mit seinen sechs Phasen Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation und Deployment. Pfeile markieren die zentralen Abhängigkeiten, während der äußere Kreis den iterativen, zyklischen Charakter des Vorgehens unterstreicht. Das Modell ist als allgemeiner Rahmen entworfen und unabhängig von Anwendungsdomäne oder eingesetzter Software.


![Das CRISP-DM-Modell nach @wirthCRISPDMStandardProcess2000, S. 5](img/CRISP-DM.png){#fig-crisp-dm width=70%}

Jede der sechs Phasen ist in spezifische Aufgaben untergliedert, die den Ablauf systematisieren und nachvollziehbar machen. So umfasst Data Understanding das Beschreiben, Erkunden und Überprüfen der Datenqualität, während Data Preparation Aufgaben wie Auswählen, Bereinigen, Konstruieren und Integrieren von Daten vorsieht. Modeling beinhaltet die Wahl geeigneter Verfahren, die Erstellung eines Testdesigns sowie den Aufbau und die Bewertung von Modellen. Evaluation konzentriert sich auf die Überprüfung der Ergebnisse im Hinblick auf die Projektziele, und Deployment behandelt die Bereitstellung und Nutzung der Modelle im Anwendungskontext. Wirth und Hipp betonen außerdem den stark iterativen Charakter des Modells: Ergebnisse einer Phase können jederzeit zu Rücksprüngen führen, wodurch die Arbeitsschritte flexibel angepasst werden können (@wirthCRISPDMStandardProcess2000, S. 6–9).

::: {#sec-anhangA2}
## A.2: Das CRISP-DM-Modell angewendet auf diese Studienarbeit {.unnumbered}
:::

Die @fig-crisp-dm-Studienarbeit zeigt, wie die einzelnen Phasen des CRISP-DM-Modells konkret auf die Schritte und Kapitel dieser Studienarbeit abgebildet wurden. Die Zuordnung verdeutlicht, welche Arbeitsschritte in welcher Phase umgesetzt und wie sie im Dokument strukturiert sind.

![Eigene Darstellung des angewendeten CRISP-DM-Frameworks](img/CRISP-DM-Studienarbeit.png){#fig-crisp-dm-Studienarbeit width=100%}

Die Grafik ordnet die Kapitel und Arbeitsschritte dieser Studienarbeit den einzelnen Phasen des CRISP-DM-Prozesses zu. So wird sichtbar, wie die theoretischen und praktischen Teile entlang des etablierten Data-Mining-Standards strukturiert und umgesetzt wurden.


::: {#sec-anhangB}
# B: Kundensegmentierung {.unnumbered}
:::
\markboth{Anhang B: Kundensegmentierung}{}

::: {#sec-anhangB1}
## B.1: Einordnung der Kundensegmentierung unter Unsupervised Learning {.unnumbered}
:::

Die Kundensegmentierung ist ein klassisches Beispiel für Clustering und damit ein zentrales Verfahren des unüberwachten Lernens. Im Machine-Learning-Kontext werden Kunden anhand ihrer Merkmale in Gruppen eingeteilt, ohne dass eine Zielvariable vorgegeben ist. Die folgende Abbildung zeigt die Einordnung von Clustering und Kundensegmentierung innerhalb der verschiedenen Typen von Machine Learning.

![Ansatzpunkte zur Lösung nach @chakrabortyDataClassificationIncremental2022, S. 28](img/types_of_ml_algorithms.png){#fig-types_of_ml_algorithms width=100%}

\noindent Wie in der @fig-types_of_ml_algorithms dargestellt, ist Clustering ein Teilbereich des Unsupervised Learning. Kundensegmentierung nutzt Clustering-Algorithmen, um natürliche Gruppen von Kunden zu identifizieren. Diese Segmentierung bildet die Grundlage für gezielte Marketingmaßnahmen und eine personalisierte Kundenansprache, ohne dass vorher festgelegte Zielvariablen benötigt werden. Im CRM-Kontext ermöglicht dies eine datengetriebene und flexible Einteilung der Kundenbasis.

\clearpage

::: {#sec-anhangC}
# C: Erste Analyse der Daten {.unnumbered}
:::
\markboth{Anhang C: Erste Analyse der Daten}{}

\lorem


::: {#sec-anhangC1}
## C.1: Lorem {.unnumbered}
:::

\lorem









<!-- ================= Ende ================= -->


