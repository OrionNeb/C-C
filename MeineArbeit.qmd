---
lang: de  # Dokumentensprache: Deutsch

format:
  pdf:
    documentclass: book
    classoption: openany, oneside  # Kapitel auf jeder Seite starten (verhindert leere Seiten)
    mainfont: "Latin Modern Roman"
    fontsize: 11pt
    linestretch: 1.5  # Zeilenabstand auf 1,5 setzen (für bessere Lesbarkeit)
    code-fold: false  # Code-Chunks bleiben immer sichtbar (nicht einklappbar)
    toc: false
    toc-depth: 4
    number-sections: true
    unnumbered-sections: true
    geometry: "top=3cm, bottom=3cm, headheight=15pt, headsep=1.5cm, footskip=1.5cm"
    pdf-engine: lualatex
    listings: false
    highlight-style: tango  # Syntax-Highlighting für Code
    code-block-font-size: \tiny
    code-overflow: wrap
    include-in-header: 
      - setup/header.qmd
      - setup/glossar-eintraege.qmd
    include-before-body:
      - setup/titlepage.qmd
      - setup/gendererklaerung.qmd
      - setup/abstract.qmd
      - setup/verzeichnisse.qmd
      - setup/mainmatter.qmd
    include-after-body:
      - setup/glossar.qmd
      - setup/ehenwoertlicheErklaerung.qmd
  html:
    code-fold: true  # Code-Chunks in HTML können ausgeklappt werden
    toc: false
    toc-depth: 4
    number-sections: true
    self-contained: true  # Alle Ressourcen (Bilder, CSS, etc.) in einer Datei speichern
    other-formats: ["pdf", "docx", "pptx"]
  docx:
    toc: false
    number-sections: true
  pptx:
    reference-doc: setup/FHDW-Powerpoint-Vorlage_16zu9.pptx  # (Optional) Eigene Vorlage nutzen
bibliography: setup/references.yaml
citeproc: true
csl: setup/apa-with-abstract-de.csl # https://www.zotero.org/styles/apa-with-abstract
link-citations: true
link-bibliography: true
editor: source
execute:
  freeze: auto
  eval: true
  echo: true       # Code anzeigen
  results: markup  # Output als formatierten Text ausgeben
  warning: false   # Warnungen unterdrücken
  message: false   # Nachrichten (z. B. von Paketen) unterdrücken
  code-overflow: wrap  # Lange Zeilen umbrechen
---

```{r eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}
# Installiere und lade erforderliche Bibliotheken
if (!requireNamespace("reticulate", quietly = TRUE)) install.packages("reticulate")
if (!requireNamespace("rmarkdown", quietly = TRUE)) install.packages("rmarkdown")
if (!requireNamespace("shiny", quietly = TRUE)) install.packages("shiny")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("tidyr", quietly = TRUE)) install.packages("tidyr")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
if (!requireNamespace("forecast", quietly = TRUE)) install.packages("forecast")
if (!requireNamespace("ggpubr", quietly = TRUE)) install.packages("ggpubr")
# if (!requireNamespace("viridis", quietly = TRUE)) install.packages("viridis")



library(reticulate)
library(rmarkdown)
library(shiny)
library(dplyr)
library(tidyr)
library(tidyverse)
library(forecast)
library(ggpubr)
# library(viridis)
```

```{python, include=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Erforderliche Bibliotheken installieren (falls nicht vorhanden)
import subprocess
import sys

def install_and_import(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = __import__(package)

# Äquivalente Bibliotheken in Python installieren und importieren
libraries = ["pandas", "numpy", "matplotlib", "seaborn", "scipy"]

for lib in libraries:
    install_and_import(lib)

# Bibliotheken laden
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.seasonal import STL
from pulp import LpMaximize, LpProblem, LpVariable, value

```



<!-- ================= 1. Einleitung ================= -->

# Einleitung {#sec-einleitung}

## Problemstellung {#sec-problemstellung}

Unternehmen segmentieren Kunden oft je Datenquelle, etwa nur im Clickstream oder nur in Transaktionen. So entstehen Cluster mit begrenzter Übertragbarkeit. CRM benötigt jedoch stabile, interpretierbare Segmente, die Nutzungs- und Kaufverhalten zusammenführen und ohne personenbezogene Daten auskommen.



## Zielsetzung {#sec-zielsetzung}

Die Arbeit vereinheitlicht RetailRocket und Online Retail II zu einem Ereignisschema und beschreibt Kunden mit einem gemeinsamen Merkmalset aus RFM, Konversionsraten, Zeitmustern, Diversität, Zwischenkaufintervallen sowie Text-Mining auf Kategoriepfaden. Darauf aufbauend werden K-Means und HDBSCAN angewandt und verglichen. Qualität, Stabilität und Clustertendenz werden mit Silhouette-Index, Davies-Bouldin-Index, DBCV, Bootstrapping, Adjusted Rand Index und der Hopkins-Statistik bewertet.



## Vorgehensweise {#sec-vorgehensweise}

Die Untersuchung folgt CRISP-DM mit Datenverständnis und EDA, Vereinheitlichung, Merkmalserstellung, Modellierung, Evaluation und Profiling zu CRM-Segmenten. Leitend ist die Frage, wie robust und übertragbar Kundencluster aus RetailRocket und Online Retail II sind, die mit K-Means und HDBSCAN gebildet werden, gemessen mit Silhouette-Index, Davies-Bouldin-Index, DBCV und Adjusted Rand Index, und wie sie zu geschäftlich interpretierbaren Segmenten profiliert werden können.




<!-- ================= 2. Theoretische Grundlagen ================= -->

# Grundlagen {#sec-grundlagen}

CRISP-DM bildet den Prozessrahmen dieser Arbeit. Wirth und Hipp schreiben: „The life cycle of a data mining project is broken down in six phases“ (@wirthCRISPDMStandardProcess2000, S. 4). Die Phasen reichen von Business Understanding über Data Understanding, Data Preparation, Modeling und Evaluation bis zu Deployment; sie werden iterativ durchlaufen (@wirthCRISPDMStandardProcess2000, S. 4–7). Wirth und Hipp sprechen von „this highly iterative, creative process with many parallel activities“ und halten fest: „it is never the case that a phase is completely done before the subsequent phase starts“ (@wirthCRISPDMStandardProcess2000, S. 9). Dieser Rahmen stellt sicher, dass Ziele, Daten, Modelle und Bewertung systematisch aufeinander aufbauen.

## Kundensegmentierung im CRM Kontext {#sec-kundensegmentierung}

Clustering wird im CRM als Verfahren des unüberwachten Lernens eingesetzt, das natürliche Gruppierungen erkennt und Kunden in intern kohäsive Gruppen einordnet. „Clustering techniques identify meaningful natural groupings of records and group customers into distinct segments with internal cohesion“ (@tsiptsisDataMiningTechniques2010, S. 39). Ziel ist die Bildung unterscheidbarer Kundentypologien, „so that they can be marketed more effectively“ (@tsiptsisDataMiningTechniques2010, S. 40). Der Nutzen einer Segmentierung misst sich daran, ob die resultierenden Typologien transparent, aussagekräftig und handlungsleitend sind. „The value of each solution depends on its ability to represent transparent, meaningful, and actionable customer typologies“ (@tsiptsisDataMiningTechniques2010, S. 129).

## Zwei unabhängige Datensichten und deren Harmonisierung im Lichte von CRISP-DM {#sec-crisp-dm}

Business Understanding konkretisiert das Ziel auf robuste und übertragbare Segmente. Data Understanding führt zwei unabhängige, aber komplementäre Datensichten ein: eine verhaltensnahe Ereignissicht und eine wertbasierte Rechnungssicht. Erst die Kombination erlaubt zu prüfen, ob Clusterstrukturen auf gleich definierten Merkmalen in unterschiedlichen Datenwelten wiederkehren und damit eher domänenweit gültige Muster darstellen als datensatzspezifische Effekte (@wirthCRISPDMStandardProcess2000, S. 5; @tsiptsisDataMiningTechniques2010, S. 39–40).

In Data Preparation werden beide Quellen in ein gemeinsames Ereignisschema überführt, um Messäquivalenz herzustellen. Praktisch umfasst dies konsistente Felder wie customer_id, item_id, timestamp und event_type sowie bei Bedarf quantity, price, revenue und eine Kategoriendarstellung. Damit lassen sich zentrale Konstrukte wie Recency, Frequency, Interkaufintervalle, Diversität und Zeitmuster in beiden Datensichten identisch operationalisieren (@wirthCRISPDMStandardProcess2000, S. 5–6).

In Modeling werden komplementäre Verfahren auf denselben Kernmerkmalen je Datensicht kalibriert, etwa ein zentroidbasiertes und ein dichtebasiertes Verfahren, um unterschiedliche Strukturannahmen abzudecken (@wirthCRISPDMStandardProcess2000, S. 6). Die Evaluation kombiniert interne Qualitätsmaße, die zur jeweiligen Methode passen, mit Stabilitäts- und Übertragbarkeitstests zwischen den Datensichten, bevor Ergebnisse reproduzierbar dokumentiert werden (Evaluation und Deployment, @wirthCRISPDMStandardProcess2000, S. 6–7, S. 9).

## Datenquellen und Datenschutz RetailRocket und Online Retail II {#sec-datenquellen}

Beide Datensätze sind offen zugänglich und anonymisiert, sodass in dieser Arbeit keine personenbezogenen Daten verarbeitet werden. RetailRocket bildet verhaltensnahe Ereignisfolgen ab, Online Retail II die wertbasierte Rechnungssicht; die Detailbeschreibung folgt erst im Praxisteil. Dieser kurze Abschnitt begründet lediglich die Verwendbarkeit und verankert die Quellen im Prozessrahmen.

## Verfahren und Qualitätsmaße im Überblick K Means, HDBSCAN und passende Indizes {#sec-verfahren-qualitaet}

K Means steht als zentroidbasierte Baseline für kompakte Strukturen, HDBSCAN als dichtebasierter Ansatz für ungleich dichte Cluster und Rauschen. Bewertet wird mit zu den Verfahren passenden internen Indizes und mit Stabilitätstests; entscheidend ist die spätere Übertragbarkeit zwischen den Datensichten. Formelhafte Details und Parametertabellen entfallen hier und erscheinen erst mit den Ergebnissen.

## Übergang zum Praxisteil Messäquivalenz und minimaler Merkmalssatz {#sec-uebergang}

Für einen fairen Vergleich werden beide Quellen in ein gemeinsames Ereignisschema überführt und ein identischer Kern von Merkmalen definiert. Dieser minimale, quellenübergreifende Merkmalssatz trägt die Modellierung und die Stabilitätsanalysen, während zusätzliche merkmals- oder datensatzspezifische Details erst in Kapitel 3 genutzt und begründet werden. So bleibt der Grundlagenteil schlank und führt direkt in die praktische Umsetzung.



<!-- ================= 3. Praxisteil ================= -->

# Praxisteil

## Datenverständnis RetailRocket Ereignisschema und Eigenschaften {#sec-datenverstaendnisRR}

## Datenverständnis Online Retail II Transaktionsschema und Eigenschaften {#sec-datenverstaendnisOR}

## Vereinheitlichung beider Datensätze gemeinsames Ereignisschema {#sec-datenvereinheitlichung}

## Datenaufbereitung und Feature Engineering RFM Konversionsproxys Zeitmuster Diversität Interkaufintervalle {#sec-datenaufbereitung}

## Text Mining auf Kategoriepfaden TF IDF optional Item2Vec {#sec-textmining}

## Clustering Ansatz 1 K-Means Auswahl der Clusterzahl über Silhouette und Davies Bouldin {#sec-clustering1}

## Clustering Ansatz 2 HDBSCAN Parameterwahl und DBCV {#sec-clustering2}

## Stabilität und Robustheit Bootstrapping und Adjusted Rand Index {#sec-stabilitaet}

## Segmentprofiling KPI Profile und Top Kategorien je Cluster {#sec-segmentprofiling}

## Reproduzierbarkeit Notebook Struktur Hyperparameter Versionierung {#sec-reproduzierbarkeit}

<!-- ================= 4. Evaluation und Ergebnisse ================= -->

# Evaluation und Ergebnisse

## Clusterqualität je Datensatz RetailRocket und Online Retail II {#sec-clusterqualitaet}

## Vergleich der Lösungen K-Means gegenüber HDBSCAN {#sec-vergleich}

## Sensitivitätsanalyse Feature Varianten und Hyperparameter {#sec-sensitivitaetsanalyse}

## Grenzen Datenqualität Sparsity Cold Start {#sec-grenzen}

<!-- ================= 5. Fazit und Ausblick ================= -->

# Fazit und Ausblick



Was eine \gls{mfa} ist, wird im Glossar beschrieben. Auch glspl und glslink sind möglich.


## Beantwortung der Forschungsfrage
## Nutzen für CRM, Kampagnensteuerung und nächste Schritte
## Ausblick weitere Text Mining Merkmale und Uplift Experimente

<!-- ===
# Anhang

## A1 Prompts und Antworten der KI Hilfsmittel
## A2 Datenschemata RetailRocket und Online Retail II
## A3 Zusätzliche Tabellen zu Qualitätsmaßen und Bootstraps
## A4 Codeauszüge und Umgebungsparameter
## A5 Zusätzliche Grafiken
=== -->

<!-- ================= Literaturverzeichnis ================= -->

# Literaturverzeichnis {.unnumbered}

::: {#refs}
:::
\pagestyle{literature}
\markboth{Literaturverzeichnis}{Literaturverzeichnis}

\clearpage

<!-- ================= Anhangsverzeichnis-Header ================= -->

::: {#sec-anhang}
# Anhang {.unnumbered}
:::
\markboth{Anhang}{Anhang} 
\thispagestyle{empty}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0}



::: {#sec-anhangsverzeichnis}
## Anhangsverzeichnis {.unnumbered}
:::
\pagestyle{appendixTOC}
\noindent
<!-- ================= Anhangsverzeichnis ================= -->


**A** [Zero-Trust-Modell](#sec-anhangA) \dotfill  \pageref{sec-anhangA}

&nbsp;&nbsp;&nbsp;**A.1** [Zero-Trust-Säulen](#sec-anhangA1) \dotfill  \pageref{sec-anhangA1}

**B** [Erste Analyse der Daten](#sec-anhangB) \dotfill  \pageref{sec-anhangB}

&nbsp;&nbsp;&nbsp;**B.1** [Ergebnisse der ersten Analyse](#sec-anhangB1) \dotfill  \pageref{sec-anhangB1}
 





\clearpage

<!-- ================= Anhang ================= -->
\pagestyle{appendix} 
::: {#sec-anhangA}
# A: Zero-Trust-Modell {.unnumbered}
:::
\markboth{Anhang A: Zero-Trust-Modell}{}

::: {#sec-anhangA1}
## A1: Zero-Trust-Säulen {.unnumbered}
:::

\lorem

\clearpage

::: {#sec-anhangB}
# B: Erste Analyse der Daten {.unnumbered}
:::
\markboth{Anhang B: Erste Analyse der Daten}{}

\lorem


::: {#sec-anhangB1}
## B1: Lorem {.unnumbered}
:::

\lorem









<!-- ================= Ende ================= -->


