---
lang: de  # Dokumentensprache: Deutsch

format:
  pdf:
    documentclass: book
    classoption: openany, oneside  # Kapitel auf jeder Seite starten (verhindert leere Seiten)
    mainfont: "Latin Modern Roman"
    fontsize: 11pt
    linestretch: 1.5  # Zeilenabstand auf 1,5 setzen (für bessere Lesbarkeit)
    code-fold: false  # Code-Chunks bleiben immer sichtbar (nicht einklappbar)
    toc: false
    toc-depth: 4
    number-sections: true
    unnumbered-sections: true
    geometry: "top=3cm, bottom=3cm, headheight=15pt, headsep=1.5cm, footskip=1.5cm"
    pdf-engine: lualatex
    listings: false
    highlight-style: tango  # Syntax-Highlighting für Code
    code-block-font-size: \tiny
    code-overflow: wrap
    include-in-header: 
      - setup/header.qmd
      - setup/glossar-eintraege.qmd
    include-before-body:
      - setup/titlepage.qmd
      - setup/gendererklaerung.qmd
      - setup/abstract.qmd
      - setup/verzeichnisse.qmd
      - setup/mainmatter.qmd
    include-after-body:
      - setup/glossar.qmd
      - setup/ehenwoertlicheErklaerung.qmd
  html:
    code-fold: true  # Code-Chunks in HTML können ausgeklappt werden
    toc: false
    toc-depth: 4
    number-sections: true
    self-contained: true  # Alle Ressourcen (Bilder, CSS, etc.) in einer Datei speichern
    other-formats: ["pdf", "docx", "pptx"]
  docx:
    toc: false
    number-sections: true
  pptx:
    reference-doc: setup/FHDW-Powerpoint-Vorlage_16zu9.pptx  # (Optional) Eigene Vorlage nutzen
bibliography: setup/references.yaml
citeproc: true
csl: setup/apa-with-abstract-de.csl # https://www.zotero.org/styles/apa-with-abstract
link-citations: true
link-bibliography: true
editor: source
execute:
  freeze: auto
  eval: true
  echo: true       # Code anzeigen
  results: markup  # Output als formatierten Text ausgeben
  warning: false   # Warnungen unterdrücken
  message: false   # Nachrichten (z. B. von Paketen) unterdrücken
  code-overflow: wrap  # Lange Zeilen umbrechen
---



```{r eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}
# Installiere und lade erforderliche Bibliotheken
if (!requireNamespace("reticulate", quietly = TRUE)) install.packages("reticulate")
if (!requireNamespace("rmarkdown", quietly = TRUE)) install.packages("rmarkdown")
if (!requireNamespace("shiny", quietly = TRUE)) install.packages("shiny")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("tidyr", quietly = TRUE)) install.packages("tidyr")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
if (!requireNamespace("forecast", quietly = TRUE)) install.packages("forecast")
if (!requireNamespace("ggpubr", quietly = TRUE)) install.packages("ggpubr")
# if (!requireNamespace("viridis", quietly = TRUE)) install.packages("viridis")




library(reticulate)
library(rmarkdown)
library(shiny)
library(dplyr)
library(tidyr)
library(tidyverse)
library(forecast)
library(ggpubr)
# library(viridis)
```

```{python, include=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Erforderliche Bibliotheken installieren (falls nicht vorhanden)
import subprocess
import sys

def install_and_import(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = __import__(package)

# Äquivalente Bibliotheken in Python installieren und importieren
libraries = ["pandas", "numpy", "matplotlib", "seaborn", "scipy"]

for lib in libraries:
    install_and_import(lib)

# Bibliotheken laden
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.seasonal import STL
from pulp import LpMaximize, LpProblem, LpVariable, value

```



<!-- ================= 1. Einleitung ================= -->

# Einleitung {#sec-einleitung}

## Problemstellung {#sec-problemstellung}

Unternehmen segmentieren Kunden oft je Datenquelle, etwa nur im Clickstream oder nur in Transaktionen. So entstehen Cluster mit begrenzter Übertragbarkeit. CRM benötigt jedoch stabile, interpretierbare Segmente, die Nutzungs- und Kaufverhalten zusammenführen und ohne personenbezogene Daten auskommen.


## Zielsetzung {#sec-zielsetzung}

Die Arbeit vereinheitlicht RetailRocket und Online Retail II zu einem Ereignisschema und beschreibt Kunden mit einem gemeinsamen Merkmalset aus RFM, Konversionsraten, Zeitmustern, Diversität, Zwischenkaufintervallen sowie Text-Mining auf Kategoriepfaden. Darauf aufbauend werden K-Means und HDBSCAN angewandt und verglichen. Qualität, Stabilität und Clustertendenz werden mit Silhouette-Index, Davies-Bouldin-Index, DBCV, Bootstrapping, Adjusted Rand Index und der Hopkins-Statistik bewertet.



## Vorgehensweise {#sec-vorgehensweise}

Die Untersuchung folgt CRISP-DM mit Datenverständnis und EDA, Vereinheitlichung, Merkmalserstellung, Modellierung, Evaluation und Profiling zu CRM-Segmenten. Leitend ist die Frage, wie robust und übertragbar Kundencluster aus RetailRocket und Online Retail II sind, die mit K-Means und HDBSCAN gebildet werden, gemessen mit Silhouette-Index, Davies-Bouldin-Index, DBCV und Adjusted Rand Index, und wie sie zu geschäftlich interpretierbaren Segmenten profiliert werden können.




<!-- ================= 2. Theoretische Grundlagen ================= -->

# Grundlagen {#sec-grundlagen}

CRISP-DM bildet den Prozessrahmen dieser Arbeit. Wirth und Hipp schreiben: „The life cycle of a data mining project is broken down in six phases“ (@wirthCRISPDMStandardProcess2000, S. 4). Die Phasen reichen von Business Understanding über Data Understanding, Data Preparation, Modeling und Evaluation bis zu Deployment; sie werden iterativ durchlaufen (@wirthCRISPDMStandardProcess2000, S. 4–7). Wirth und Hipp sprechen von „this highly iterative, creative process with many parallel activities“ und halten fest: „it is never the case that a phase is completely done before the subsequent phase starts“ (@wirthCRISPDMStandardProcess2000, S. 9). Dieser Rahmen stellt sicher, dass Ziele, Daten, Modelle und Bewertung systematisch aufeinander aufbauen.

## Kundensegmentierung im CRM Kontext {#sec-kundensegmentierung}

Clustering wird im CRM als Verfahren des unüberwachten Lernens eingesetzt, das natürliche Gruppierungen erkennt und Kunden in intern kohäsive Gruppen einordnet. „Clustering techniques identify meaningful natural groupings of records and group customers into distinct segments with internal cohesion“ (@tsiptsisDataMiningTechniques2010, S. 39). Ziel ist die Bildung unterscheidbarer Kundentypologien, „so that they can be marketed more effectively“ (@tsiptsisDataMiningTechniques2010, S. 40). Der Nutzen einer Segmentierung misst sich daran, ob die resultierenden Typologien transparent, aussagekräftig und handlungsleitend sind. „The value of each solution depends on its ability to represent transparent, meaningful, and actionable customer typologies“ (@tsiptsisDataMiningTechniques2010, S. 129). Als visuelle Einordnung dient im [Anhang B - Kundensegmentierung](#sec-anhangB), Abschnitt [Anhang B.1 - Einordnung der Kundensegmentierung unter Unsupervised Learning](#sec-anhangB1) die Lernarten-Übersicht, in der Kundensegmentierung unter Unsupervised Learning verortet ist.

## Zwei unabhängige Datensichten und deren Harmonisierung im Lichte von CRISP-DM {#sec-crisp-dm}

Business Understanding konkretisiert das Ziel auf robuste und übertragbare Segmente. Data Understanding führt zwei unabhängige, aber komplementäre Datensichten ein: eine verhaltensnahe Ereignissicht und eine wertbasierte Rechnungssicht. Erst die Kombination erlaubt zu prüfen, ob Clusterstrukturen auf gleich definierten Merkmalen in unterschiedlichen Datenwelten wiederkehren und damit eher domänenweit gültige Muster darstellen als datensatzspezifische Effekte (@wirthCRISPDMStandardProcess2000, S. 5; @tsiptsisDataMiningTechniques2010, S. 39–40).

In Data Preparation werden beide Quellen in ein gemeinsames Ereignisschema überführt, um Messäquivalenz herzustellen. Praktisch umfasst dies konsistente Felder wie customer_id, item_id, timestamp und event_type sowie bei Bedarf quantity, price, revenue und eine Kategoriendarstellung. Damit lassen sich zentrale Konstrukte wie Recency, Frequency, Interkaufintervalle, Diversität und Zeitmuster in beiden Datensichten identisch operationalisieren (@wirthCRISPDMStandardProcess2000, S. 5–6).

In Modeling werden komplementäre Verfahren auf denselben Kernmerkmalen je Datensicht kalibriert, etwa ein zentroidbasiertes und ein dichtebasiertes Verfahren, um unterschiedliche Strukturannahmen abzudecken (@wirthCRISPDMStandardProcess2000, S. 6). Die Evaluation kombiniert interne Qualitätsmaße, die zur jeweiligen Methode passen, mit Stabilitäts- und Übertragbarkeitstests zwischen den Datensichten, bevor Ergebnisse reproduzierbar dokumentiert werden (@wirthCRISPDMStandardProcess2000, S. 6–7, S. 9).

## Datenquellen und Datenschutz RetailRocket und Online Retail II {#sec-datenquellen}

Diese Studie nutzt zwei offen zugängliche, anonymisierte E-Commerce-Datensätze mit komplementärer Sicht. Der RetailRocket-Datensatz wurde vom Anbieter auf Kaggle veröffentlicht und enthält roh erfasste Shop-Ereignisse sowie artikelbezogene Eigenschaften und den Kategoriebaum. Zentrale Dateien sind events.csv mit den Ereignistypen view, addtocart und transaction, item_properties_part1.csv, item_properties_part2.csv sowie category_tree.csv. Identifikatoren für Nutzer und Artikel sind gehasht; personenbezogene Daten liegen nicht vor. Die Kaggle-Seite gilt als Primärreferenz des Datensatzes (@zykovRetailrocketRecommenderSystem2022). Online Retail II ist eine von UCI kuratierte Transaktionshistorie eines britischen Händlers mit Produktcodes, Mengen, Preisen, Zeitstempeln und einer Kundenkennung. Inhaltlich wird die UCI-Landingpage zitiert; der praktische Download erfolgte über den Kaggle-Mirror (@DatasetsUCIMachine2019; @miyabonOnlineRetailII2009). Für diese Arbeit genügt ein knapper Datenschutzhinweis, da ausschließlich anonymisierte Forschungsdaten verwendet werden.

## Verfahren und Qualitätsmaße im Überblick K Means, HDBSCAN und passende Indizes {#sec-verfahren-qualitaet}

K-Means dient in dieser Arbeit als zentroidbasierte Baseline. Die folgenden Zitate belegen genau die Punkte, die für unser Vorgehen entscheidend sind Iterationsprinzip und Zielgröße, Vorabwahl der Clusterzahl und Zentroidrepräsentation sowie Distanzmaß und Konvergenzkriterium. Erstens zum Iterationsprinzip und Ziel K-Means „starts with an initial cluster solution which is updated and adjusted until no further refinement is possible … Each iteration refines the solution by reducing the within-cluster variation“ (@tsiptsisDataMiningTechniques2010, S. 85). Zweitens zur Modellannahme über die Segmentzahl „The "K" in the algorithm’s name comes from the fact that users should specify in advance the number of k clusters to be formed. The "means" part of the name refers to the fact that each cluster is represented by the means of its records … the cluster central point or centroid“ (@tsiptsisDataMiningTechniques2010, S. 85). Drittens zum Ablauf und zur Konvergenz „K-means uses the Euclidean distance measure … The procedure starts by selecting k well-spaced initial records as cluster centers … This iterative procedure is repeated until it converges and the migration of records between clusters no longer refines the solution“ (@tsiptsisDataMiningTechniques2010, S. 86).
Die herangezogenen Quellen belegen präzise die methodischen Voraussetzungen dieser Studie: ein standardisierter numerischer Merkmalsraum mit euklidischer Distanz sowie die transparente Bestimmung der Clusterzahl auf Basis geeigneter Indizes. Zugleich verdeutlichen sie, warum K-Means klar interpretierbare Zentroidprofile erzeugt, die sich unmittelbar für das anschließende Segmentprofiling nutzen lassen.

HDBSCAN steht für eine hierarchische Weiterentwicklung dichtebasierter Clusterverfahren. Dichtebasierte Ansätze identifizieren Gruppen dort, wo Beobachtungen in Regionen hoher Punktdichte liegen, und behandeln spärlich besetzte Bereiche als Rauschen. Das zugrunde liegende Prinzip wird bei DBSCAN anhand der Parameter $\varepsilon$ und MinPts eingeführt; dadurch lassen sich nicht-konvexe Formen erkennen und Ausreißer explizit als noise ausweisen (@chakrabortyDataClassificationIncremental2022, S. 87–89). HDBSCAN knüpft an dieses Prinzip an, setzt jedoch auf eine hierarchische Betrachtung der Dichte, sodass Cluster über ein Spektrum von Dichteschwellen hinweg ermittelt und stabilere Gruppierungen extrahiert werden können. Damit entfällt die feste Vorgabe einer Clusterzahl und es entstehen Lösungen, die mit lokal unterschiedlichen Dichten und Rauschen umgehen können.

Der methodische Kontrast zu zentroidbasierten Verfahren ist dabei grundlegend. Während K-Means vor allem bei annähernd kugelförmigen Strukturen überzeugt, eignen sich hierarchische Verfahren für arbiträr geformte Cluster (@chakrabortyDataClassificationIncremental2022, S. 96). In dieser Arbeit wird HDBSCAN deshalb als dichtebasiertes Gegenstück zu K-Means eingesetzt; die konkrete Parametrisierung und der algorithmische Ablauf folgen im Methodenteil.

Abschließend zum Methodenvergleich ist der Umgang mit Ausreißern wichtig. In E-Commerce-Daten treten extreme Preise, Mengen oder Event-Bursts auf. K-Means reagiert darauf empfindlich, weil einzelne Extreme Zentren und Distanzen verschieben, während dichtebasierte Verfahren dünn besetzte Bereiche als Rauschen behandeln. Die zugrunde liegende Idee wird beim dichtebasierten Clustering mit $\varepsilon$ und MinPts erläutert, die Outlier-Analyse ordnet Begriffe und Vorgehen ein (@chakrabortyDataClassificationIncremental2022, S. 87–89). Darauf baut die regelbasierte Ausreißerbehandlung in Kapitel 3 auf.

## Qualitäts- und Stabilitätsmaße im Überblick {#sec-qualitaet}

Für die Modellwahl werden interne Gütemaße eingesetzt, die ohne externe Labels die Trennschärfe und Kompaktheit der gefundenen Gruppen beurteilen. Bei K-Means stehen damit vor allem metrische Indizes im Vordergrund. Die Silhouette fasst für jedes Objekt Verhältnis von Binnen- zu Zwischenabstand zusammen und liefert einen intuitiven Skalenwert von schlecht bis gut; Calinski-Harabasz bewertet die Varianzaufspaltung zwischen und innerhalb der Cluster, während Davies–Bouldin geringe mittlere Ähnlichkeiten zwischen Clusterzentren belohnt. Für dichtebasierte Lösungen mit Rauschen und nicht-konvexen Formen ist ein dichteorientiertes Kriterium zweckmäßig; daher verwenden wir einen DBCV-Index, der Kohäsion und Separation über Dichtegrößen erfasst. Ergänzend berichten wir stets den Anteil des als Rauschen eingestuften Punktemasse.
Robustheit wird über Wiederholungsstichproben geprüft: Per Bootstrapping entstehen Replikate, deren Zuordnungen mit dem Adjusted Rand Index verglichen werden. Hohe ARI-Werte deuten auf stabile Lösungen hin. Vor der Modellierung wird außerdem die Clustertendenz des Merkmalsraums mit der Hopkins-Statistik geprüft, um zu vermeiden, dass in nahezu zufällige Strukturen hinein modelliert wird. Schließlich testen wir die Übertragbarkeit, indem eine Lösung aus der Ereignissicht auf die Rechnungssicht projiziert wird und umgekehrt; neben ARI betrachten wir dabei die Ähnlichkeit der Segmentprofile. Die formalen Definitionen und Rechenregeln folgen im Methodenteil.

## Übergang zum Praxisteil im Lichte von CRISP-DM {#sec-uebergang}

Der Praxisteil setzt die bisherige Einordnung entlang der CRISP-DM-Phasen um. Ausgehend vom Business Understanding wird die Forschungsfrage nach robusten und übertragbaren Kundensegmenten präzisiert. In Data Understanding werden die beiden offenen E-Commerce-Quellen charakterisiert, Clustertendenz und Datenqualität geprüft und ein geeignetes Beobachtungsfenster festgelegt. Data Preparation harmonisiert beide Quellen in ein gemeinsames Ereignisschema und stellt Messäquivalenz her, sodass zentrale Konstrukte wie Recency, Frequency, Monetary, Interkaufintervalle, Diversität oder Zeitmuster identisch berechnet werden können. Darauf aufbauend erfolgt die Modeling-Phase mit zwei komplementären Verfahren K-Means als zentroidbasierte Baseline und HDBSCAN als hierarchisch dichtebasiertes Gegenstück. Die Evaluation nutzt die in Abschnitt 2.5 skizzierten Qualitäts- und Stabilitätsmaße und erweitert sie um Übertragbarkeitstests zwischen den Datensichten; alle Schritte werden reproduzierbar dokumentiert. CRISP-DM versteht diesen Ablauf ausdrücklich als iterativen, phasenübergreifenden Prozess, in dem Ergebnisse einer Phase die nächste anstoßen und bei Bedarf Rücksprünge erlauben (@wirthCRISPDMStandardProcess2000, S. 4–7, 9). Damit ist der Übergang zur praktischen Umsetzung vorbereitet.






<!-- ================= 3. Praxisteil ================= -->

# Praxisteil

## Datenverständnis RetailRocket Ereignisschema und Eigenschaften {#sec-datenverstaendnisRR}

## Datenverständnis Online Retail II Transaktionsschema und Eigenschaften {#sec-datenverstaendnisOR}

## Vereinheitlichung beider Datensätze gemeinsames Ereignisschema {#sec-datenvereinheitlichung}

## Datenaufbereitung und Feature Engineering RFM Konversionsproxys Zeitmuster Diversität Interkaufintervalle {#sec-datenaufbereitung}

## Text Mining auf Kategoriepfaden TF IDF optional Item2Vec {#sec-textmining}

## Clustering Ansatz 1 K-Means Auswahl der Clusterzahl über Silhouette und Davies Bouldin {#sec-clustering1}

## Clustering Ansatz 2 HDBSCAN Parameterwahl und DBCV {#sec-clustering2}

## Stabilität und Robustheit Bootstrapping und Adjusted Rand Index {#sec-stabilitaet}

## Segmentprofiling KPI Profile und Top Kategorien je Cluster {#sec-segmentprofiling}

## Reproduzierbarkeit Notebook Struktur Hyperparameter Versionierung {#sec-reproduzierbarkeit}

<!-- ================= 4. Evaluation und Ergebnisse ================= -->

# Evaluation und Ergebnisse

## Clusterqualität je Datensatz RetailRocket und Online Retail II {#sec-clusterqualitaet}

## Vergleich der Lösungen K-Means gegenüber HDBSCAN {#sec-vergleich}

## Sensitivitätsanalyse Feature Varianten und Hyperparameter {#sec-sensitivitaetsanalyse}

## Grenzen Datenqualität Sparsity Cold Start {#sec-grenzen}

<!-- ================= 5. Fazit und Ausblick ================= -->

# Fazit und Ausblick



Was eine \gls{mfa} ist, wird im Glossar beschrieben. Auch glspl und glslink sind möglich.


## Beantwortung der Forschungsfrage
## Nutzen für CRM, Kampagnensteuerung und nächste Schritte
## Ausblick weitere Text Mining Merkmale und Uplift Experimente

<!-- ===
# Anhang

## A1 Prompts und Antworten der KI Hilfsmittel
## A2 Datenschemata RetailRocket und Online Retail II
## A3 Zusätzliche Tabellen zu Qualitätsmaßen und Bootstraps
## A4 Codeauszüge und Umgebungsparameter
## A5 Zusätzliche Grafiken
=== -->

<!-- ================= Literaturverzeichnis ================= -->

# Literaturverzeichnis {.unnumbered}

::: {#refs}
:::
\pagestyle{literature}
\markboth{Literaturverzeichnis}{Literaturverzeichnis}

\clearpage

<!-- ================= Anhangsverzeichnis-Header ================= -->

::: {#sec-anhang}
# Anhang {.unnumbered}
:::
\markboth{Anhang}{Anhang} 
\thispagestyle{empty}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0}



::: {#sec-anhangsverzeichnis}
## Anhangsverzeichnis {.unnumbered}
:::
\pagestyle{appendixTOC}
\noindent
<!-- ================= Anhangsverzeichnis ================= -->


**A** [Kundensegmentierung](#sec-anhangA) \dotfill  \pageref{sec-anhangA}

&nbsp;&nbsp;&nbsp;**A.1** [Einordnung der Kundensegmentierung unter Unsupervised Learning](#sec-anhangA1) \dotfill  \pageref{sec-anhangA1}

**B** [Erste Analyse der Daten](#sec-anhangB) \dotfill  \pageref{sec-anhangB}

&nbsp;&nbsp;&nbsp;**B.1** [Ergebnisse der ersten Analyse](#sec-anhangB1) \dotfill  \pageref{sec-anhangB1}
 





\clearpage

<!-- ================= Anhang ================= -->
\pagestyle{appendix} 
::: {#sec-anhangA}
# A: Kundensegmentierung {.unnumbered}
:::
\markboth{Anhang A: Kundensegmentierung}{}

::: {#sec-anhangA1}
## A1: Einordnung der Kundensegmentierung unter Unsupervised Learning {.unnumbered}
:::

Die Kundensegmentierung ist ein klassisches Beispiel für Clustering und damit ein zentrales Verfahren des unüberwachten Lernens. Im Machine-Learning-Kontext werden Kunden anhand ihrer Merkmale in Gruppen eingeteilt, ohne dass eine Zielvariable vorgegeben ist. Die folgende Abbildung zeigt die Einordnung von Clustering und Kundensegmentierung innerhalb der verschiedenen Typen von Machine Learning.

![Ansatzpunkte zur Lösung nach @chakrabortyDataClassificationIncremental2022, S. 28](img/types_of_ml_algorithms.png){#fig-types_of_ml_algorithms width=100%}

\noindent Wie in der @fig-types_of_ml_algorithms dargestellt, ist Clustering ein Teilbereich des Unsupervised Learning. Kundensegmentierung nutzt Clustering-Algorithmen, um natürliche Gruppen von Kunden zu identifizieren. Diese Segmentierung bildet die Grundlage für gezielte Marketingmaßnahmen und eine personalisierte Kundenansprache, ohne dass vorher festgelegte Zielvariablen benötigt werden. Im CRM-Kontext ermöglicht dies eine datengetriebene und flexible Einteilung der Kundenbasis.

\clearpage

::: {#sec-anhangB}
# B: Erste Analyse der Daten {.unnumbered}
:::
\markboth{Anhang B: Erste Analyse der Daten}{}

\lorem


::: {#sec-anhangB1}
## B1: Lorem {.unnumbered}
:::

\lorem









<!-- ================= Ende ================= -->


