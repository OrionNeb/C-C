---
lang: de  # Dokumentensprache: Deutsch

format:
  pdf:
    documentclass: book
    classoption: openany, oneside  # Kapitel auf jeder Seite starten (verhindert leere Seiten)
    mainfont: "Latin Modern Roman"
    fontsize: 11pt
    linestretch: 1.5  # Zeilenabstand auf 1,5 setzen (für bessere Lesbarkeit)
    code-fold: false  # Code-Chunks bleiben immer sichtbar (nicht einklappbar)
    toc: false
    toc-depth: 4
    number-sections: true
    unnumbered-sections: true
    geometry: "top=3cm, bottom=3cm, headheight=15pt, headsep=1.5cm, footskip=1.5cm"
    pdf-engine: lualatex
    listings: false
    highlight-style: tango  # Syntax-Highlighting für Code
    code-block-font-size: \tiny
    code-overflow: wrap
    include-in-header: 
      - setup/header.qmd
      - setup/glossar-eintraege.qmd
    include-before-body:
      - setup/titlepage.qmd
      - setup/gendererklaerung.qmd
      - setup/abstract.qmd
      - setup/verzeichnisse.qmd
      - setup/mainmatter.qmd
    include-after-body:
      - setup/glossar.qmd
      - setup/ehenwoertlicheErklaerung.qmd
  html:
    code-fold: true  # Code-Chunks in HTML können ausgeklappt werden
    toc: false
    toc-depth: 4
    number-sections: true
    self-contained: true  # Alle Ressourcen (Bilder, CSS, etc.) in einer Datei speichern
    other-formats: ["pdf", "docx", "pptx"]
  docx:
    toc: false
    number-sections: true
  pptx:
    reference-doc: setup/FHDW-Powerpoint-Vorlage_16zu9.pptx  # (Optional) Eigene Vorlage nutzen
bibliography: setup/references.yaml
citeproc: true
csl: setup/apa-with-abstract-de.csl # https://www.zotero.org/styles/apa-with-abstract
link-citations: true
link-bibliography: true
editor: source
execute:
  freeze: auto
  eval: true
  echo: true       # Code anzeigen
  results: markup  # Output als formatierten Text ausgeben
  warning: false   # Warnungen unterdrücken
  message: false   # Nachrichten (z. B. von Paketen) unterdrücken
  code-overflow: wrap  # Lange Zeilen umbrechen
---



```{r eval=TRUE, message=FALSE, warning=FALSE, include=FALSE}

# Arbeitsumgebung leeren (alle Objekte entfernen)
rm(list = ls())
gc()

# Root für Quarto setzen
knitr::opts_knit$set(root.dir = here::here())

# Installiere und lade erforderliche Bibliotheken
if (!requireNamespace("reticulate", quietly = TRUE)) install.packages("reticulate")
if (!requireNamespace("rmarkdown", quietly = TRUE)) install.packages("rmarkdown")
if (!requireNamespace("shiny", quietly = TRUE)) install.packages("shiny")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("tidyr", quietly = TRUE)) install.packages("tidyr")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
if (!requireNamespace("forecast", quietly = TRUE)) install.packages("forecast")
if (!requireNamespace("ggpubr", quietly = TRUE)) install.packages("ggpubr")
if (!requireNamespace("viridis", quietly = TRUE)) install.packages("viridis")




library(reticulate)
library(rmarkdown)
library(shiny)
library(dplyr)
library(tidyr)
library(tidyverse)
library(forecast)
library(ggpubr)
library(viridis)

```

```{python, include=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Erforderliche Bibliotheken installieren (falls nicht vorhanden)
import subprocess
import sys

def install_and_import(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = __import__(package)

# Äquivalente Bibliotheken in Python installieren und importieren
libraries = ["pandas", "numpy", "matplotlib", "seaborn", "scipy", "statsmodels", "pulp"]

for lib in libraries:
    install_and_import(lib)

# Bibliotheken laden
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.seasonal import STL
from pulp import LpMaximize, LpProblem, LpVariable, value

```



<!-- ================= 1. Einleitung ================= -->

# Einleitung {#sec-einleitung}

## Problemstellung {#sec-problemstellung}

Unternehmen segmentieren Kunden oft je Datenquelle, etwa nur im Clickstream oder nur in Transaktionen. So entstehen Cluster mit begrenzter Übertragbarkeit. CRM benötigt jedoch stabile, interpretierbare Segmente, die Nutzungs- und Kaufverhalten zusammenführen und ohne personenbezogene Daten auskommen.


## Zielsetzung {#sec-zielsetzung}

Die Arbeit vereinheitlicht RetailRocket und Online Retail II zu einem Ereignisschema und beschreibt Kunden mit einem gemeinsamen Merkmalset aus RFM, Konversionsraten, Zeitmustern, Diversität, Zwischenkaufintervallen sowie Text-Mining auf Kategoriepfaden. Darauf aufbauend werden K-Means und HDBSCAN angewandt und verglichen. Qualität, Stabilität und Clustertendenz werden mit Silhouette-Index, Davies-Bouldin-Index, DBCV, Bootstrapping, Adjusted Rand Index und der Hopkins-Statistik bewertet.



## Vorgehensweise {#sec-vorgehensweise}

Die Untersuchung folgt CRISP-DM mit Datenverständnis und EDA, Vereinheitlichung, Merkmalserstellung, Modellierung, Evaluation und Profiling zu CRM-Segmenten. Leitend ist die Frage, wie robust und übertragbar Kundencluster aus RetailRocket und Online Retail II sind, die mit K-Means und HDBSCAN gebildet werden, gemessen mit Silhouette-Index, Davies-Bouldin-Index, DBCV und Adjusted Rand Index, und wie sie zu geschäftlich interpretierbaren Segmenten profiliert werden können.




<!-- ================= 2. Theoretische Grundlagen ================= -->

# Grundlagen {#sec-grundlagen}

CRISP-DM bildet den Prozessrahmen dieser Arbeit. Wirth und Hipp schreiben: „The life cycle of a data mining project is broken down in six phases“ (@wirthCRISPDMStandardProcess2000, S. 4). Die Phasen reichen von Business Understanding über Data Understanding, Data Preparation, Modeling und Evaluation bis zu Deployment; sie werden iterativ durchlaufen (@wirthCRISPDMStandardProcess2000, S. 4–7). Wirth und Hipp sprechen von „this highly iterative, creative process with many parallel activities“ und halten fest: „it is never the case that a phase is completely done before the subsequent phase starts“ (@wirthCRISPDMStandardProcess2000, S. 9). Dieser Rahmen stellt sicher, dass Ziele, Daten, Modelle und Bewertung systematisch aufeinander aufbauen und ist in [Anhang A: CRISP-DM](#sec-anhangA), [Abschnitt A1: Das CRISP-DM-Modell](#sec-anhangA1) grafisch dargestellt.

## Kundensegmentierung im CRM Kontext {#sec-kundensegmentierung}

Clustering wird im CRM als Verfahren des unüberwachten Lernens eingesetzt, das natürliche Gruppierungen erkennt und Kunden in intern kohäsive Gruppen einordnet. „Clustering techniques identify meaningful natural groupings of records and group customers into distinct segments with internal cohesion“ (@tsiptsisDataMiningTechniques2010, S. 39). Ziel ist die Bildung unterscheidbarer Kundentypologien, „so that they can be marketed more effectively“ (@tsiptsisDataMiningTechniques2010, S. 40). Der Nutzen einer Segmentierung misst sich daran, ob die resultierenden Typologien transparent, aussagekräftig und handlungsleitend sind. „The value of each solution depends on its ability to represent transparent, meaningful, and actionable customer typologies“ (@tsiptsisDataMiningTechniques2010, S. 129). Als visuelle Einordnung dient im [Anhang B: Kundensegmentierung](#sec-anhangB), Abschnitt [Anhang B.1: Einordnung der Kundensegmentierung unter Unsupervised Learning](#sec-anhangB1) die Lernarten-Übersicht, in der Kundensegmentierung unter Unsupervised Learning verortet ist.

## Zwei unabhängige Datensichten und deren Harmonisierung im Lichte von CRISP-DM {#sec-crisp-dm}

Business Understanding konkretisiert das Ziel auf robuste und übertragbare Segmente. Data Understanding führt zwei unabhängige, aber komplementäre Datensichten ein: eine verhaltensnahe Ereignissicht und eine wertbasierte Rechnungssicht. Erst die Kombination erlaubt zu prüfen, ob Clusterstrukturen auf gleich definierten Merkmalen in unterschiedlichen Datenwelten wiederkehren und damit eher domänenweit gültige Muster darstellen als datensatzspezifische Effekte (@wirthCRISPDMStandardProcess2000, S. 5; @tsiptsisDataMiningTechniques2010, S. 39–40).

In Data Preparation werden beide Quellen in ein gemeinsames Ereignisschema überführt, um Messäquivalenz herzustellen. Praktisch umfasst dies konsistente Felder wie customer_id, item_id, timestamp und event_type sowie bei Bedarf quantity, price, revenue und eine Kategoriendarstellung. Damit lassen sich zentrale Konstrukte wie Recency, Frequency, Interkaufintervalle, Diversität und Zeitmuster in beiden Datensichten identisch operationalisieren (@wirthCRISPDMStandardProcess2000, S. 5–6).

In Modeling werden komplementäre Verfahren auf denselben Kernmerkmalen je Datensicht kalibriert, etwa ein zentroidbasiertes und ein dichtebasiertes Verfahren, um unterschiedliche Strukturannahmen abzudecken (@wirthCRISPDMStandardProcess2000, S. 6). Die Evaluation kombiniert interne Qualitätsmaße, die zur jeweiligen Methode passen, mit Stabilitäts- und Übertragbarkeitstests zwischen den Datensichten, bevor Ergebnisse reproduzierbar dokumentiert werden (@wirthCRISPDMStandardProcess2000, S. 6–7, S. 9).

## Datenquellen und Datenschutz RetailRocket und Online Retail II {#sec-datenquellen}

Diese Studie nutzt zwei offen zugängliche, anonymisierte E-Commerce-Datensätze mit komplementärer Sicht. Der RetailRocket-Datensatz wurde vom Anbieter auf Kaggle veröffentlicht und enthält roh erfasste Shop-Ereignisse sowie artikelbezogene Eigenschaften und den Kategoriebaum. Zentrale Dateien sind events.csv mit den Ereignistypen view, addtocart und transaction, item_properties_part1.csv, item_properties_part2.csv sowie category_tree.csv. Identifikatoren für Nutzer und Artikel sind gehasht; personenbezogene Daten liegen nicht vor. Die Kaggle-Seite gilt als Primärreferenz des Datensatzes (@zykovRetailrocketRecommenderSystem2022). Online Retail II ist eine von UCI kuratierte Transaktionshistorie eines britischen Händlers mit Produktcodes, Mengen, Preisen, Zeitstempeln und einer Kundenkennung. Inhaltlich wird die UCI-Landingpage zitiert; der praktische Download erfolgte über den Kaggle-Mirror (@DatasetsUCIMachine2019; @miyabonOnlineRetailII2009). Für diese Arbeit genügt ein knapper Datenschutzhinweis, da ausschließlich anonymisierte Forschungsdaten verwendet werden.

## Verfahren und Qualitätsmaße im Überblick K Means, HDBSCAN und passende Indizes {#sec-verfahren-qualitaet}

K-Means dient in dieser Arbeit als zentroidbasierte Baseline. Die folgenden Zitate belegen genau die Punkte, die für unser Vorgehen entscheidend sind Iterationsprinzip und Zielgröße, Vorabwahl der Clusterzahl und Zentroidrepräsentation sowie Distanzmaß und Konvergenzkriterium. Erstens zum Iterationsprinzip und Ziel K-Means „starts with an initial cluster solution which is updated and adjusted until no further refinement is possible … Each iteration refines the solution by reducing the within-cluster variation“ (@tsiptsisDataMiningTechniques2010, S. 85). Zweitens zur Modellannahme über die Segmentzahl „The "K" in the algorithm’s name comes from the fact that users should specify in advance the number of k clusters to be formed. The "means" part of the name refers to the fact that each cluster is represented by the means of its records … the cluster central point or centroid“ (@tsiptsisDataMiningTechniques2010, S. 85). Drittens zum Ablauf und zur Konvergenz „K-means uses the Euclidean distance measure … The procedure starts by selecting k well-spaced initial records as cluster centers … This iterative procedure is repeated until it converges and the migration of records between clusters no longer refines the solution“ (@tsiptsisDataMiningTechniques2010, S. 86).
Die herangezogenen Quellen belegen präzise die methodischen Voraussetzungen dieser Studie: ein standardisierter numerischer Merkmalsraum mit euklidischer Distanz sowie die transparente Bestimmung der Clusterzahl auf Basis geeigneter Indizes. Zugleich verdeutlichen sie, warum K-Means klar interpretierbare Zentroidprofile erzeugt, die sich unmittelbar für das anschließende Segmentprofiling nutzen lassen.

HDBSCAN steht für eine hierarchische Weiterentwicklung dichtebasierter Clusterverfahren. Dichtebasierte Ansätze identifizieren Gruppen dort, wo Beobachtungen in Regionen hoher Punktdichte liegen, und behandeln spärlich besetzte Bereiche als Rauschen. Das zugrunde liegende Prinzip wird bei DBSCAN anhand der Parameter $\varepsilon$ und MinPts eingeführt; dadurch lassen sich nicht-konvexe Formen erkennen und Ausreißer explizit als noise ausweisen (@chakrabortyDataClassificationIncremental2022, S. 87–89). HDBSCAN knüpft an dieses Prinzip an, setzt jedoch auf eine hierarchische Betrachtung der Dichte, sodass Cluster über ein Spektrum von Dichteschwellen hinweg ermittelt und stabilere Gruppierungen extrahiert werden können. Damit entfällt die feste Vorgabe einer Clusterzahl und es entstehen Lösungen, die mit lokal unterschiedlichen Dichten und Rauschen umgehen können.

Der methodische Kontrast zu zentroidbasierten Verfahren ist dabei grundlegend. Während K-Means vor allem bei annähernd kugelförmigen Strukturen überzeugt, eignen sich hierarchische Verfahren für arbiträr geformte Cluster (@chakrabortyDataClassificationIncremental2022, S. 96). In dieser Arbeit wird HDBSCAN deshalb als dichtebasiertes Gegenstück zu K-Means eingesetzt; die konkrete Parametrisierung und der algorithmische Ablauf folgen im Methodenteil.

Abschließend zum Methodenvergleich ist der Umgang mit Ausreißern wichtig. In E-Commerce-Daten treten extreme Preise, Mengen oder Event-Bursts auf. K-Means reagiert darauf empfindlich, weil einzelne Extreme Zentren und Distanzen verschieben, während dichtebasierte Verfahren dünn besetzte Bereiche als Rauschen behandeln. Die zugrunde liegende Idee wird beim dichtebasierten Clustering mit $\varepsilon$ und MinPts erläutert, die Outlier-Analyse ordnet Begriffe und Vorgehen ein (@chakrabortyDataClassificationIncremental2022, S. 87–89). Darauf baut die regelbasierte Ausreißerbehandlung in Kapitel 3 auf.

## Qualitäts- und Stabilitätsmaße im Überblick {#sec-qualitaet}

Zur Bewertung der Clusterlösungen werden für jedes Verfahren passende Gütemaße verwendet. Für K-Means liegt der Schwerpunkt auf dem Silhouettenkoeffizienten. Dieses Maß erfasst, wie gut ein Objekt zu seinem eigenen Cluster passt, verglichen mit der Nähe zu anderen Clustern. Der Wert reicht von –1 bis +1, wobei höhere Werte auf klar abgegrenzte Clusterstrukturen hinweisen. Tsiptsis und Chorianopoulos nennen die Silhouette als ein zentrales Kriterium für die technische Evaluation von Clustern (@tsiptsisDataMiningTechniques2010, S. 209). Lai et al. betonen, dass der Silhouettenkoeffizient sowohl die Kohäsion innerhalb der Cluster als auch die Trennung zwischen Clustern berücksichtigt und damit ein objektives Maß für die Güte der Gruppierung liefert (@laiSilhouetteCoefficientbasedWeighting2025, S. 3063).

Für dichtebasierte Verfahren wie HDBSCAN eignet sich der DBCV-Index. Moulavi et al. betonen, dass „DBCV employs the concept of Hartigan’s model of density-contour trees to compute the least dense region inside a cluster and the most dense region between the clusters, which are used to measure the within and between-cluster density connectedness of clusters“ (@moulaviDensityBasedClusteringValidation2014, S. 839). Damit wird die Qualität einer Clustering-Lösung nicht mehr über Abstände zwischen Zentren gemessen, sondern über dichtebasierte Eigenschaften. Ein guter Cluster hat dabei eine höhere Mindestdichte im Inneren als die maximale Dichte, die ihn von anderen Clustern trennt. Zudem stellen die Autoren klar: „Unlike other relative validity indices, our method not only directly takes into account density and shape properties of clusters but also properly deals with noise objects, which are intrinsic to the definition of the density-based clustering“ (@moulaviDensityBasedClusteringValidation2014, S. 845). Das bedeutet, dass das Verfahren auch Ausreißer und Rauschen korrekt berücksichtigt, die bei dichtebasierten Algorithmen wie HDBSCAN typischerweise auftreten. So lässt sich die Qualität der Lösung auch dann zuverlässig beurteilen, wenn die Cluster keine einfachen, kugelförmigen Strukturen bilden.

Ergänzend zur reinen Gütebewertung wird die Stabilität geprüft. Hierzu werden wiederholt Stichproben aus den Daten gezogen, Modelle neu geschätzt und die Übereinstimmung der Clusterzuordnungen mit dem Adjusted Rand Index gemessen. Werte nahe bei 1 deuten auf eine stabile Lösung hin. Vor der eigentlichen Modellierung wird zudem mit der Hopkins-Statistik getestet, ob der Datensatz überhaupt eine ausgeprägte Clustertendenz enthält. Werte nahe 0,5 sprechen für Zufälligkeit, während Werte über 0,75 auf deutliche Clusterstrukturen hindeuten.

Ergänzend zur reinen Gütebewertung wird die Stabilität der Clusterlösungen untersucht. Dazu werden wiederholt Stichproben aus den Daten gezogen und die Modelle neu geschätzt. Hennig beschreibt dieses Vorgehen als naheliegend: „The Jaccard coefficient, a similarity measure between sets, is used as a cluster-wise measure of cluster stability, which is assessed by the bootstrap distribution of the Jaccard coefficient for every single cluster of a clustering compared to the most similar cluster in the bootstrapped data sets“ (@hennigClusterwiseAssessmentCluster2007, S. 258). Auf Ebene der gesamten Clusterlösung wird häufig der Adjusted Rand Index eingesetzt. Hennig weist darauf hin, dass „the adjusted Rand index (Hubert and Arabie, 1985) has been used often to measure the similarity between two complete clusterings“ (@hennigClusterwiseAssessmentCluster2007, S. 260). Auch Moulavi et al. nennen den Adjusted Rand Index als gängiges Maß, um Clustering-Ergebnisse mit einer bestehenden Struktur zu vergleichen: „External clustering validity approaches such as the Adjusted Rand Index compare clustering results with a pre-existing clustering (or class) structure, i.e., a ground truth solution“ (@moulaviDensityBasedClusteringValidation2014, S. 840). Werte nahe bei 1 deuten in beiden Fällen auf eine stabile Lösung hin. 

Vor der eigentlichen Modellierung wird zudem geprüft, ob die Daten überhaupt eine ausgeprägte Clustertendenz enthalten. Ein etabliertes Maß hierfür ist die Hopkins-Statistik. Sie „can be used to assess the clustering tendency of a data set by measuring the probability that a uniform data distribution generates a given data set. In other words, it considers the spatial randomness of the data“ (@acitoPredictiveAnalyticsKNIME2023, S. 271). Werte nahe 0,5 sprechen für eine zufällige Verteilung, während Werte über 0,75 auf deutliche Clusterstrukturen schließen lassen.


## Übergang zum Praxisteil im Lichte von CRISP-DM {#sec-uebergang}

Die folgenden Schritte orientieren sich eng am CRISP-DM-Referenzmodell und werden iterativ durchlaufen. Wirth und Hipp betonen: „The sequence of the phases is not strict. The arrows indicate only the most important and frequent dependencies between phases, but in a particular project, it depends on the outcome of each phase which phase, or which particular task of a phase, has to be performed next“ (@wirthCRISPDMStandardProcess2000, S. 4).  
Genau diese flexible und zyklische Struktur nutzen wir, um Datenaufbereitung, Merkmalsbildung und Modellwahl eng mit den Evaluationsbefunden zu koppeln.

Konkret bedeutet das für den Praxisteil: Zunächst vertiefen wir Data Understanding für beide Quellen und legen Datenqualität, Beobachtungsfenster und Clustertendenz fest. In Data Preparation harmonisieren wir RetailRocket und Online Retail II in ein gemeinsames Ereignisschema, sodass RFM-, Zeit- und Diversitätsmerkmale messäquivalent berechnet werden. In Modeling setzen wir zwei komplementäre Verfahren ein: K-Means als zentroidbasierte Baseline und HDBSCAN als hierarchisch dichtebasiertes Gegenstück. Die Evaluation folgt dem in Abschnitt [Qualitäts- und Stabilitätsmaße im Überblick](#sec-qualitaet) beschriebenen Set aus Silhouette, Calinski-Harabasz, Davies-Bouldin bzw. DBCV, ergänzt um Bootstrapping und ARI zur Stabilität sowie den Rauschanteil bei HDBSCAN.

Zur Orientierung, wie diese Schritte in dieser Arbeit konkret den CRISP-DM-Phasen zugeordnet sind, verweist [Anhang A: CRISP-DM](#sec-anhangA), [Abschnitt A.2: Das CRISP-DM-Modell angewendet auf diese Studienarbeit](#sec-anhangA2).

<!-- ================= 3. Praxisteil ================= -->

# Methodik und Umsetzung {#sec-methodik}

## Datenbasis, Harmonisierung und Bereinigung {#sec-datenbasis}

Die Analyse basiert auf zwei offen zugänglichen, anonymisierten E-Commerce-Datensätzen, die unterschiedliche Perspektiven auf das Kundenverhalten eröffnen. \medskip 

  - RetailRocket enthält ereignisbasierte Daten aus einem Online-Shop. Erfasst werden Interaktionen wie Seitenaufrufe, Warenkorbaktionen und Transaktionen. Ergänzende Dateien ordnen Artikeln Eigenschaften und Kategorien zu.
  - Online Retail II stellt transaktionsbasierte Rechnungsinformationen eines britischen Händlers bereit. Enthalten sind unter anderem Rechnungsnummer, Artikelcode, Menge, Preis, Zeitstempel und eine anonymisierte Kundenkennung.  

\medskip

\noindent Damit die Daten gemeinsam ausgewertet werden können, ist eine Harmonisierung der Strukturen erforderlich. Geplant ist die Überführung der Rohdaten in ein gemeinsames Ereignisschema, in dem zentrale Variablen wie Kunden-, Artikel- und Zeitinformationen konsistent abgebildet werden. Außerdem wird eine grundlegende Bereinigung vorgesehen, bei der Duplikate und fehlerhafte Einträge behandelt sowie Rückgaben eindeutig gekennzeichnet werden.  

Die detaillierte Variablenbeschreibung findet sich im [Anhang C.1: Variablenbeschreibung und Quellen](#sec-anhangC1). Die Abbildung der heterogenen Strukturen auf ein einheitliches Ereignisschema ist im [Anhang C.2: Abbildung auf ein einheitliches Ereignisschema](#sec-anhangC2) dokumentiert. Eine zusammenfassende Darstellung aller Arbeitsschritte zur Datenaufbereitung befindet sich im [Anhang C](#sec-anhangC).




## Gemeinsames Merkmalset RFM, Konversionsproxys, Zeitmuster, Intervalle, Diversität {#sec-merkmalset}

Auf Grundlage des harmonisierten Ereignisschemas werden aus den Rohdaten abgeleitete Merkmale erstellt, die als Input für die Clusterverfahren dienen. Ziel ist es, sowohl wertorientierte als auch verhaltensorientierte Aspekte des Kundenverhaltens abzubilden.  

Die Merkmalsbildung umfasst insbesondere folgende Bereiche:  

  - **Recency, Frequency, Monetary (RFM):** klassische Kennzahlen aus dem Kundenwertmanagement zur Beschreibung der Aktualität, Häufigkeit und Höhe von Käufen.
  - **Konversionsproxys:** Verhältnis von Ansichten und Warenkorbaktionen zu tatsächlichen Käufen (nur im RetailRocket-Datensatz abbildbar).
  - **Zeitmuster:** Ableitung bevorzugter Kauf- und Nutzungszeiten (z. B. Tageszeit, Wochentag, Saison).
  - **Kaufintervalle:** Berechnung der Abstände zwischen aufeinanderfolgenden Käufen je Kunde.
  - **Diversität:** Variation der gekauften bzw. angesehenen Artikelkategorien als Maß für das Breiten- oder Spezialistenverhalten.  

\medskip

\noindent Die konkrete Berechnung dieser Merkmale erfordert vorbereitende Schritte wie die Bereinigung der Rohdaten und die Vereinheitlichung der Zeitangaben. Diese sind im [Anhang C.3: Bereinigungsschritte](#sec-anhangC3) sowie im [Anhang C.4: Normalisierung der Zeitstempel](#sec-anhangC4) dokumentiert. Eine zusammenfassende Darstellung des bereinigten Bestands findet sich in [C.5](#sec-anhangC5).  

\noindent Mit diesem Merkmalsset wird ein konsistenter Input für die Modellierung geschaffen, der sowohl die Interaktionssicht aus RetailRocket als auch die Transaktionssicht aus Online Retail II integriert.


## Clustertendenz und Modelle Hopkins, K-Means, HDBSCAN {#sec-clustertendenz}

## Evaluationsdesign Silhouette, CH, DB, DBCV, Bootstrapping, ARI {#sec-evaluationsdesign}

<!-- ================= 4. Evaluation und Ergebnisse ================= -->

# Evaluation und Ergebnisse {#sec-evaluation}

## Qualitätsübersicht RetailRocket und Online Retail II {#sec-qualitaetsuebersicht}

## Stabilität und Robustheit ARI und Rauschanteil {#sec-stabilitaet}

## Vergleich K-Means vs. HDBSCAN {#sec-vergleich}

## Segmentprofile und CRM-Interpretation {#sec-segmentprofile}

## Limitationen und Übertragbarkeit {#sec-limitationen}

<!-- ================= 5. Fazit und Ausblick ================= -->

# Fazit und Ausblick {#sec-fazit}

## Beantwortung der Forschungsfrage {#sec-forschungsfrage}

Die Untersuchung zeigt, dass sich auf dem gemeinsamen Merkmalset in beiden Datensichten robuste und interpretierbare Kundensegmente ableiten lassen. K-Means liefert bei annähernd kugelförmigen Strukturen die höheren Silhouettenwerte und klarere Zentroidprofile, HDBSCAN identifiziert zusätzlich dichtebasierte Cluster und weist Rauschen explizit aus. Insgesamt bestätigt sich die Übertragbarkeit zentraler Muster zwischen RetailRocket und Online Retail II, wenn Merkmaldefinitionen harmonisiert und Skalierungen konsistent sind.

## Nutzen für CRM Kampagnensteuerung und nächste Schritte {#sec-crm-nutzen}

Die resultierenden Segmente sind operabel: Sie unterscheiden sich in RFM-Profilen, Kaufintervallen, Zeitmustern und Top-Kategorien. Daraus lassen sich Zielgruppenregeln für Kampagnen ableiten etwa Reaktivierung bei langen Intervallen, Cross-Sell auf Basis von TF-IDF-Kategorien, sowie Taktung nach bevorzugten Zeitfenstern. Nächste Schritte sind die Anbindung an Kampagnenlogik und ein A/B-Rahmen zur Wirkungsmessung.

## Ausblick weitere Text Mining Merkmale und Uplift Experimente {#sec-ausblick}

Zur Verfeinerung eignen sich zusätzliche Text-Merkmale aus Titel, Beschreibung und Kategoriepfaden etwa n-Gramme, Embeddings oder Item2Vec. Für die Wirksamkeitseinordnung empfehlen sich Uplift-Experimente, um Segmente mit kausalem Mehrwert zu identifizieren und Ressourcen dorthin zu priorisieren.

Was eine \gls{mfa} ist, wird im Glossar beschrieben. Auch glspl und glslink sind möglich.


<!-- ================= Literaturverzeichnis ================= -->

# Literaturverzeichnis {.unnumbered}

::: {#refs}
:::
\pagestyle{literature}
\markboth{Literaturverzeichnis}{Literaturverzeichnis}

\clearpage

<!-- ================= Anhangsverzeichnis-Header ================= -->

::: {#sec-anhang}
# Anhang {.unnumbered}
:::
\markboth{Anhang}{Anhang} 
\thispagestyle{empty}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0}



::: {#sec-anhangsverzeichnis}
## Anhangsverzeichnis {.unnumbered}
:::
\pagestyle{appendixTOC}
\noindent
<!-- ================= Anhangsverzeichnis ================= -->

**A** [CRISP-DM](#sec-anhangA) \dotfill  \pageref{sec-anhangA}

&nbsp;&nbsp;&nbsp;**A.1** [Das CRISP-DM-Modell](#sec-anhangA1) \dotfill  \pageref{sec-anhangA1}

&nbsp;&nbsp;&nbsp;**A.2** [Das CRISP-DM-Modell angewendet auf diese Studienarbeit](#sec-anhangA2) \dotfill  \pageref{sec-anhangA2}

**B** [Kundensegmentierung](#sec-anhangB) \dotfill  \pageref{sec-anhangB}

&nbsp;&nbsp;&nbsp;**B.1** [Einordnung der Kundensegmentierung unter Unsupervised Learning](#sec-anhangB1) \dotfill  \pageref{sec-anhangB1}

**C** [Erste Analyse der Daten](#sec-anhangC) \dotfill  \pageref{sec-anhangC}

&nbsp;&nbsp;&nbsp;**C.1** [Ergebnisse der ersten Analyse](#sec-anhangC1) \dotfill  \pageref{sec-anhangC1}
 





\clearpage

<!-- ================= Anhang ================= -->
\pagestyle{appendix} 

::: {#sec-anhangA}
# A: CRISP-DM {.unnumbered}
:::
\markboth{Anhang A: CRISP-DM}{}

::: {#sec-anhangA1}
## A.1: Das CRISP-DM-Modell {.unnumbered}
:::

Die @fig-crisp-dm zeigt das CRISP-DM-Referenzmodell mit seinen sechs Phasen Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation und Deployment. Pfeile markieren die zentralen Abhängigkeiten, während der äußere Kreis den iterativen, zyklischen Charakter des Vorgehens unterstreicht. Das Modell ist als allgemeiner Rahmen entworfen und unabhängig von Anwendungsdomäne oder eingesetzter Software.


![Das CRISP-DM-Modell nach @wirthCRISPDMStandardProcess2000, S. 5](img/CRISP-DM.png){#fig-crisp-dm width=70%}

Jede der sechs Phasen ist in spezifische Aufgaben untergliedert, die den Ablauf systematisieren und nachvollziehbar machen. So umfasst Data Understanding das Beschreiben, Erkunden und Überprüfen der Datenqualität, während Data Preparation Aufgaben wie Auswählen, Bereinigen, Konstruieren und Integrieren von Daten vorsieht. Modeling beinhaltet die Wahl geeigneter Verfahren, die Erstellung eines Testdesigns sowie den Aufbau und die Bewertung von Modellen. Evaluation konzentriert sich auf die Überprüfung der Ergebnisse im Hinblick auf die Projektziele, und Deployment behandelt die Bereitstellung und Nutzung der Modelle im Anwendungskontext. Wirth und Hipp betonen außerdem den stark iterativen Charakter des Modells: Ergebnisse einer Phase können jederzeit zu Rücksprüngen führen, wodurch die Arbeitsschritte flexibel angepasst werden können (@wirthCRISPDMStandardProcess2000, S. 6–9).

::: {#sec-anhangA2}
## A.2: Das CRISP-DM-Modell angewendet auf diese Studienarbeit {.unnumbered}
:::

Die @fig-crisp-dm-Studienarbeit zeigt, wie die einzelnen Phasen des CRISP-DM-Modells konkret auf die Schritte und Kapitel dieser Studienarbeit abgebildet wurden. Die Zuordnung verdeutlicht, welche Arbeitsschritte in welcher Phase umgesetzt und wie sie im Dokument strukturiert sind.

![Eigene Darstellung des angewendeten CRISP-DM-Frameworks](img/CRISP-DM-Studienarbeit.png){#fig-crisp-dm-Studienarbeit width=100%}

Die Grafik ordnet die Kapitel und Arbeitsschritte dieser Studienarbeit den einzelnen Phasen des CRISP-DM-Prozesses zu. So wird sichtbar, wie die theoretischen und praktischen Teile entlang des etablierten Data-Mining-Standards strukturiert und umgesetzt wurden.


::: {#sec-anhangB}
# B: Kundensegmentierung {.unnumbered}
:::
\markboth{Anhang B: Kundensegmentierung}{}

::: {#sec-anhangB1}
## B.1: Einordnung der Kundensegmentierung unter Unsupervised Learning {.unnumbered}
:::

Die Kundensegmentierung ist ein klassisches Beispiel für Clustering und damit ein zentrales Verfahren des unüberwachten Lernens. Im Machine-Learning-Kontext werden Kunden anhand ihrer Merkmale in Gruppen eingeteilt, ohne dass eine Zielvariable vorgegeben ist. Die folgende Abbildung zeigt die Einordnung von Clustering und Kundensegmentierung innerhalb der verschiedenen Typen von Machine Learning.

![Ansatzpunkte zur Lösung nach @chakrabortyDataClassificationIncremental2022, S. 28](img/types_of_ml_algorithms.png){#fig-types_of_ml_algorithms width=100%}

\noindent Wie in der @fig-types_of_ml_algorithms dargestellt, ist Clustering ein Teilbereich des Unsupervised Learning. Kundensegmentierung nutzt Clustering-Algorithmen, um natürliche Gruppen von Kunden zu identifizieren. Diese Segmentierung bildet die Grundlage für gezielte Marketingmaßnahmen und eine personalisierte Kundenansprache, ohne dass vorher festgelegte Zielvariablen benötigt werden. Im CRM-Kontext ermöglicht dies eine datengetriebene und flexible Einteilung der Kundenbasis.

\clearpage

::: {#sec-anhangC}
# C: Datenaufbereitung und Voranalysen {.unnumbered}
:::
\markboth{Anhang C: Datenaufbereitung und Voranalysen}{}

Die im Haupttext (vgl. Abschnitt [3.1 Datenbasis, Harmonisierung und Bereinigung](#sec-datenbasis)) skizzierten Arbeitsschritte werden in diesem Anhang detailliert dokumentiert. Dadurch bleibt der Fließtext der Studienarbeit fokussiert, während Nachvollziehbarkeit und Reproduzierbarkeit gewährleistet sind.  

\medskip

\noindent Der Anhang ist in fünf Teile gegliedert: \smallskip

  1. [Variablenbeschreibung und Quellen (C.1)](#sec-anhangC1)
  2. [Abbildung auf ein einheitliches Ereignisschema (C.2)](#sec-anhangC2)
  3. [Bereinigungsschritte (C.3)](#sec-anhangC3)
  4. [Normalisierung der Zeitstempel (C.4)](#sec-anhangC4)
  5. [Zusammenfassung des bereinigten Datenbestands (C.5)](#sec-anhangC5)

\medskip

\noindent In den Abschnitten [C.1](#sec-anhangC1) bis [C.4](#sec-anhangC4) wird ausschließlich R eingesetzt, um die Daten einzulesen, zu prüfen, zu bereinigen und in ein konsistentes Ereignisschema zu überführen. Mit Paketen wie dplyr, tidyr und here lassen sich die Rohdaten strukturiert verarbeiten und tabellarisch dokumentieren.  

\noindent Python kommt erst im weiteren Verlauf der Arbeit (ab [Kapitel 4, Evaluation und Ergebnisse](#sec-evaluation)) zum Einsatz, um die eigentliche Modellierung der Kundensegmente mit Clustering-Algorithmen (K-Means, HDBSCAN) und die Berechnung der Qualitätsmaße umzusetzen.  

\medskip

\noindent Diese Aufgabenteilung folgt der Praxis vieler Forschungsarbeiten. R bietet klare Vorteile bei der transparenten Dokumentation und Datenaufbereitung, während Python durch seine umfangreichen Bibliotheken im Machine-Learning-Bereich für die Modellierung prädestiniert ist.

::: {#sec-anhangC1}
## C.1 Variablenbeschreibung und Quellen {.unnumbered}
:::

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# Pakete laden
library(readr)
library(dplyr)
library(here)

# Versionen dokumentieren
pkgs <- c("readr","dplyr","here")
vers <- sapply(pkgs, function(p) as.character(utils::packageVersion(p)))
attached <- pkgs %in% sub("^package:", "", search())
data.frame(package = pkgs, version = vers, attached = attached)

# Pfade bauen
p_events <- here("data","RR","events.csv")
p_item1  <- here("data","RR","item_properties_part1.csv")
p_item2  <- here("data","RR","item_properties_part2.csv")
p_cat    <- here("data","RR","category_tree.csv")
p_or2    <- if (file.exists(here("data","OR2","online_retail_II.csv"))) {
              here("data","OR2","online_retail_II.csv")
            } else {
              here("data","OR2","online_retail_ll.csv")
            }

# Laden
events        <- read_csv(p_events, show_col_types = FALSE)
item_props_1  <- read_csv(p_item1,  show_col_types = FALSE)
item_props_2  <- read_csv(p_item2,  show_col_types = FALSE)
category_tree <- read_csv(p_cat,    show_col_types = FALSE)
online_retail <- read_csv(p_or2,    show_col_types = FALSE)

# Erste 5 Zeilen je Datensatz
head(events, 5)
head(item_props_1, 5)
head(item_props_2, 5)
head(category_tree, 5)
head(online_retail, 5)
```

\bigskip

\noindent Die Ausgaben verdeutlichen die Struktur der Datensätze. Zur besseren Übersicht werden die wichtigsten Variablen und ihre Bedeutungen zusätzlich tabellarisch dargestellt:

| Variable             | Quelle                          | Bedeutung                                                                 |
|----------------------|---------------------------------|---------------------------------------------------------------------------|
| `timestamp`          | RetailRocket (events)           | Zeitstempel in Millisekunden seit 1970, Angabe des Ereigniszeitpunkts     |
| `visitorid`          | RetailRocket (events)           | Anonymisierte Kennung eines Besuchers                                     |
| `event`              | RetailRocket (events)           | Typ des Ereignisses: *view*, *addtocart*, *transaction*                   |
| `itemid`             | RetailRocket (events)           | Artikelkennung                                                            |
| `transactionid`      | RetailRocket (events)           | ID der Transaktion (optional, nur bei Käufen)                             |
| `property`           | RetailRocket (item_properties)  | Attribut eines Artikels (z. B. Kategorie, Preis, technische Merkmale)     |
| `value`              | RetailRocket (item_properties)  | Wert des jeweiligen Attributs                                             |
| `categoryid`         | RetailRocket (category_tree)    | Identifikator einer Kategorie                                             |
| `parentid`           | RetailRocket (category_tree)    | Übergeordnete Kategorie, beschreibt die Hierarchie                        |
| `Invoice`            | Online Retail II                | Rechnungsnummer                                                           |
| `StockCode`          | Online Retail II                | Artikelcode                                                               |
| `Description`        | Online Retail II                | Kurzbeschreibung des Artikels                                             |
| `Quantity`           | Online Retail II                | Menge der verkauften Artikel                                              |
| `InvoiceDate`        | Online Retail II                | Zeitstempel der Rechnungsstellung                                         |
| `Price`              | Online Retail II                | Einzelpreis pro Artikel                                                   |
| `Customer ID`         | Online Retail II                | Anonymisierte Kundenkennung                                               |
| `Country`            | Online Retail II                | Land des Kunden                                                           |

: Übersicht der Variablen in beiden Datenquellen {#tbl-variablen}

\medskip

\noindent Damit ist nachvollziehbar, dass RetailRocket eine ereignisbasierte Sicht (Nutzung und Interaktion) bereitstellt, während Online Retail II eine transaktionsbasierte Sicht liefert. Beide Perspektiven ergänzen sich und bilden die Grundlage für die in C.2 dargestellte Harmonisierung in ein gemeinsames Ereignisschema.

::: {#sec-anhangC2}
## C.2: Abbildung auf ein einheitliches Ereignisschema {.unnumbered}
:::

Ziel ist eine gemeinsame Struktur beider Quellen, in der Kunden-, Artikel-, Zeit- und Ereignisinformationen identisch benannt sind. Zusätzliche Felder für Menge, Preis, Umsatz und Kategorie werden bereits angelegt, auch wenn sie in RetailRocket zunächst leer bleiben. Preise und Kategorien werden dort später aus *item_properties* und *category_tree* ergänzt (vgl. [C.3 Bereinigungsschritte](#sec-anhangC3), [C.4 Normalisierung der Zeitstempel](#sec-anhangC4)).


Zuordnung der Quell- zu den Zielvariablen

| Zielvariable            | RetailRocket (Quelle)             | Online Retail II (Quelle) | Bedeutung                                          |
|-------------------------|-----------------------------------|---------------------------|----------------------------------------------------|
| `customer_id`           | `visitorid`                       | `Customer ID`              | Anonymisierte Kennung des Kunden                   |
| `item_id`               | `itemid`                          | `StockCode`               | Artikelkennung                                     |
| `timestamp`             | `timestamp` (ms seit 1970)        | `InvoiceDate`             | Zeitpunkt des Ereignisses / der Transaktion        |
| `event_type`            | `event` (*view*, *addtocart*, *transaction*)      | implizit: immer *purchase* | Typ des Ereignisses               |
| `transactionid`         | `transactionid` (falls Kauf)      | `Invoice`                 | Eindeutige Transaktionskennung                     |
| `quantity`              | nicht enthalten                   | `Quantity`                | Anzahl der verkauften Artikel                      |
| `price`                 | über `item_properties`            | `Price`                   | Einzelpreis pro Artikel                            |
| `revenue`               | berechnet: quantity × price       | berechnet: Quantity × Price | Umsatz pro Ereignis                              |
| `category`              | aus `category_tree`               | ggf. aus `Description`    | Zuordnung zu einer Produktkategorie                |

: Abbildung der Variablen auf das Ereignisschema {#tbl-ereignisschema}

\medskip

\noindent Während RetailRocket eine feingranulare Unterscheidung nach Ereignistypen erlaubt (*view*, *addtocart*, *transaction*), bildet Online Retail II ausschließlich abgeschlossene Käufe ab. Beide Datensätze liefern somit unterschiedliche, aber komplementäre Informationen.  

Die Abbildung in ein gemeinsames Schema stellt sicher, dass spätere Merkmalsberechnungen (z. B. Recency, Frequency, Monetary-Werte, Kaufintervalle, Diversität) auf konsistenter Datenbasis erfolgen können.  

Die Umsetzung der Transformation erfolgt mit R. Der folgende Chunk zeigt exemplarisch, wie die Zielvariablen aus beiden Quellen generiert werden.


```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# RetailRocket auf Ereignisschema abbilden
rr_events <- events %>%
  select(customer_id = visitorid,
         item_id     = itemid,
         timestamp   = timestamp,
         event_type  = event,
         transactionid) %>%
  mutate(timestamp   = as.POSIXct(timestamp/1000, origin="1970-01-01", tz="UTC"),
         quantity    = ifelse(event_type == "transaction", 1, NA),
         price       = NA_real_,   # später aus item_properties
         revenue     = ifelse(!is.na(quantity) & !is.na(price), quantity * price, NA_real_),
         category    = NA_character_) # später aus category_tree

# Online Retail II auf Ereignisschema abbilden
or2_events <- online_retail %>%
  select(transactionid = Invoice,
         item_id       = StockCode,
         customer_id   = `Customer ID`,
         timestamp     = InvoiceDate,
         quantity      = Quantity,
         price         = Price,
         country       = Country,
         description   = Description) %>%   # hier mit reinnehmen
  mutate(event_type = "purchase",
         revenue    = quantity * price,
         category   = description)          # hier benutzen


# Vorschau
head(rr_events, 5)
head(or2_events, 5)

```

\medskip

\noindent In der harmonisierten Tabelle von RetailRocket bleiben die Variablen `quantity`, `price`, `revenue` und `category` zunächst leer, da diese Informationen im Rohdatensatz nicht in direkter Form vorliegen. Diese Spalten wurden dennoch angelegt, um die Struktur beider Quellen konsistent zu halten und spätere Ergänzungen zu ermöglichen.  

Bei Online Retail II sind diese Variablen hingegen vollständig verfügbar: Transaktionskennungen, Mengen, Preise und Umsätze können direkt übernommen werden. Der Ereignistyp ist dort implizit immer ein Kauf (*purchase*), sodass keine Unterscheidung zwischen verschiedenen Interaktionen möglich ist.  

Die Abbildung verdeutlicht damit die komplementäre Natur der Daten: RetailRocket liefert eine feingranulare Sicht auf Nutzerverhalten, Online Retail II eine wertbasierte Sicht auf Käufe. Das vereinheitlichte Ereignisschema stellt sicher, dass in den folgenden Schritten (vgl. [C.3 Bereinigungsschritte](#sec-anhangC3) und [C.4 Normalisierung der Zeitstempel](#sec-anhangC4)) eine konsistente Datenbasis für die Merkmalsbildung geschaffen wird.



::: {#sec-anhangC3}
## C.3: Bereinigungsschritte {.unnumbered}
:::

Die Rohdaten enthalten neben den relevanten Informationen auch fehlerhafte oder problematische Einträge. Um eine konsistente Datenbasis für die spätere Merkmalsbildung zu schaffen, wurden die folgenden Regeln angewandt: \medskip

  1. Pflichtfelder: Beobachtungen ohne Kunden-, Artikel- oder Zeitangabe wurden entfernt.
  2. Duplikate: Exakte Dubletten (gleiche Kunden-, Artikel-, Zeit- und Ereignisinformationen) wurden eliminiert.
  3. Rückgaben: Im Datensatz Online Retail II wurden negative Mengen sowie Rechnungsnummern mit Präfix „C“ als Rückgaben gekennzeichnet. Diese Ereignisse erhielten den Typ `return`.
  4. Ausreißer: Mengen und Preise wurden per Winsorisierung auf das 1. und 99. Perzentil begrenzt, um extreme Werte abzuflachen.
  5. Fehlerhafte Werte: Offensichtlich ungültige Beobachtungen (z. B. Mengen ≤ 0 nach Winsorisierung) wurden ausgeschlossen.

\medskip

\noindent Im RetailRocket-Datensatz bleiben die Variablen `quantity`, `price` und `revenue` leer, da diese Informationen dort nicht in direkter Form vorliegen. Die Ereignisse konzentrieren sich auf Interaktionen wie *view*, *addtocart* und *transaction*. Im Online-Retail-II-Datensatz konnten dagegen Transaktionskennungen, Mengen, Preise und Umsätze bereinigt und in plausibler Form übernommen werden.  

Die Anwendung dieser Regeln stellt sicher, dass Auswertungen auf stabilen Grundlagen erfolgen und Verzerrungen durch fehlerhafte Einträge vermieden werden. Die bereinigten Daten bilden damit die Ausgangsbasis für die Normalisierung der Zeitangaben in Abschnitt [C.4](#sec-anhangC4).

```{r echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# benötigte Pakete
library(dplyr)
library(stringr)

# Vorbedingungen prüfen
if (!exists("rr_events"))  stop("rr_events fehlt. Bitte C.2 ausführen.")
if (!exists("or2_events")) stop("or2_events fehlt. Bitte C.2 ausführen.")

# Hilfsfunktion: sichere Winsorisierung (tut nichts, wenn zu wenig gültige Werte vorliegen)
safe_winsorize <- function(x, p_low = 0.01, p_high = 0.99) {
  if (!is.numeric(x)) return(x)
  nn <- sum(!is.na(x))
  if (nn < 10) return(x)
  lo <- suppressWarnings(quantile(x, probs = p_low, na.rm = TRUE))
  hi <- suppressWarnings(quantile(x, probs = p_high, na.rm = TRUE))
  x <- pmin(pmax(x, lo), hi)
  as.numeric(x)
}

# 1) RetailRocket bereinigen
# Pflichtfelder belegen, exakte Duplikate entfernen
rr_clean <- rr_events %>%
  # nur wirklich notwendige Felder
  select(customer_id, item_id, timestamp, event_type, transactionid,
         quantity, price, revenue, category) %>%
  # Pflichtfelder
  filter(!is.na(customer_id), !is.na(item_id), !is.na(timestamp), !is.na(event_type)) %>%
  # exakte Duplikate
  distinct(customer_id, item_id, timestamp, event_type, transactionid, .keep_all = TRUE)

# 2) Online Retail II bereinigen
# Rückgaben-Regel und Felder füllen
or2_clean <- or2_events %>%
  select(transactionid, item_id, customer_id, timestamp, quantity, price, country, description,
         event_type, revenue, category) %>%
  mutate(
    quantity = suppressWarnings(as.numeric(quantity)),
    price    = suppressWarnings(as.numeric(price))
  ) %>%
  filter(!is.na(customer_id), !is.na(item_id), !is.na(timestamp)) %>%
  mutate(
    return_flag  = (!is.na(quantity) & quantity < 0) |
                   (!is.na(transactionid) & grepl("^C", as.character(transactionid))),
    quantity_abs = if_else(return_flag & !is.na(quantity), abs(quantity), quantity),
    event_type   = if_else(return_flag, "return", "purchase"),
    category     = coalesce(category, description),
    revenue      = if_else(!is.na(quantity_abs) & !is.na(price), quantity_abs * price, revenue)
  ) %>%
  distinct(transactionid, item_id, customer_id, timestamp, quantity_abs, price, event_type, .keep_all = TRUE) %>%
  mutate(
    quantity_w = safe_winsorize(quantity_abs, 0.01, 0.99),
    price_w    = safe_winsorize(price,       0.01, 0.99),
    revenue_w  = quantity_w * price_w
  ) %>%
  filter(is.na(quantity_w) | quantity_w > 0,
         is.na(price_w)    | price_w    > 0)

# kurze Sichtprüfungen
head(rr_clean, 5)
head(or2_clean, 5)
```



\medskip

Die Vorschau zeigt, dass Pflichtfelder belegt sind, Duplikate entfernt wurden und Rückgaben in Online Retail II über `event_type = return` gekennzeichnet sind. Mengen und Preise wurden dort per Winsorisierung begrenzt und als `quantity_w` und `price_w` bereitgestellt.

::: {#sec-anhangC4}
## C.4: Normalisierung der Zeitstempel {.unnumbered}
:::

Neben inhaltlichen Bereinigungen ist auch die einheitliche Behandlung der Zeitangaben erforderlich. Beide Datensätze enthalten Zeitinformationen, die jedoch in unterschiedlicher Form vorliegen. \medskip

  - Im RetailRocket-Datensatz werden Zeitstempel in Millisekunden seit dem 1. Januar 1970 gespeichert
  - Im Online-Retail-II-Datensatz liegt das Kaufdatum bereits als Datumsfeld (InvoiceDate) vor, allerdings ohne einheitliche Ableitung von Kalendermerkmalen

\medskip

\noindent Um die Daten konsistent nutzen zu können, wurden folgende Schritte durchgeführt: \smallskip

  1. Konvertierung in ein einheitliches Format. Alle Zeitangaben wurden in das POSIXct-Format überführt und zusätzlich als Datum (date) abgespeichert.
  2. Ableitung von Kalendermerkmalen. Aus den Zeitstempeln wurden standardisierte Variablen für Wochentag (wday), Stunde (hour), Monat (month) und Jahr (year) berechnet.
  3. Harmonisierung der Skalen. Damit liegen beide Quellen auf derselben Zeitskala vor, was aggregierte Analysen und Vergleiche zwischen RetailRocket und Online Retail II ermöglicht.
  
\medskip

\noindent Die Normalisierung erlaubt es, zeitliche Muster in beiden Quellen konsistent zu erfassen, beispielsweise bevorzugte Kaufzeiten oder saisonale Effekte. Diese Variablen werden in der Merkmalsbildung (vgl. Abschnitt [3.2](#sec-merkmalset)) eingesetzt, um Kundencluster auch anhand von Verhaltensrhythmen zu unterscheiden.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# Benötigte Pakete laden
library(lubridate)
library(dplyr)

# Guard: C.3 muss gelaufen sein
if (!exists("rr_clean") || !exists("or2_clean")) {
  stop("C.4 benötigt rr_clean und or2_clean aus C.3. Bitte zuerst C.3 ausführen oder das Dokument vollständig rendern.")
}

# RetailRocket
# timestamp ist bereits POSIXct (C.2). Hier werden Datum und Kalendermerkmale ergänzt
rr_time <- rr_clean %>%
  mutate(
    # reines Datum zusätzlich ablegen
    date  = as.Date(timestamp),
    # Wochentag Mo–So, abgekürzt, Montag als Wochenbeginn
    wday  = wday(timestamp, label = TRUE, abbr = TRUE, week_start = 1),
    # Stunde 0–23
    hour  = hour(timestamp),
    # Monat Jan–Dez, abgekürzt
    month = month(timestamp, label = TRUE, abbr = TRUE),
    # Jahr vierstellig
    year  = year(timestamp)
  )

# Online Retail II
# gleiches Verfahren für Konsistenz der Skalen und Merkmale
or2_time <- or2_clean %>%
  mutate(
    date  = as.Date(timestamp),
    wday  = wday(timestamp, label = TRUE, abbr = TRUE, week_start = 1),
    hour  = hour(timestamp),
    month = month(timestamp, label = TRUE, abbr = TRUE),
    year  = year(timestamp)
  )

# Stichproben zur Kontrolle
head(rr_time, 5)
head(or2_time, 5)
```

\medskip


\noindent Die Normalisierung liegt damit für beide Datensätze in identischer Struktur vor. Die abgeleiteten Kalendermerkmale werden in der Merkmalsbildung genutzt und erlauben zeitliche Musteranalysen ohne zusätzliche Transformationen.


::: {#sec-anhangC5}
## C.5: Zusammenfassung des bereinigten Datenbestands {.unnumbered}
:::

Die Abschnitte C.3 und C.4 haben die Bereinigung und die Normalisierung der Zeitangaben dokumentiert. In diesem Abschnitt wird der bereinigte Datenbestand in komprimierter Form zusammengefasst. Dadurch lässt sich die Qualität der Datenbasis einschätzen, die in Abschnitt [3.2 Merkmalsbildung](#sec-merkmalset) für die Ableitung der Cluster-Merkmale genutzt wird.  \medskip

Zentrale Fragen der Plausibilisierung sind: \smallskip

  1. Wie groß sind die Datensätze nach der Bereinigung?
  2. Welche Verteilungen zeigen die kaufbezogenen Variablen in Online Retail II?
  3. Sind die Strukturen der beiden Quellen konsistent nutzbar?


```{r c5-zusammenfassung, dependson=c("c3-bereinigung","c4-zeit-normalisierung"), eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
library(dplyr)

# Dimensionen beider Quellen
dim_summary <- data.frame(
  datensatz  = c("RetailRocket (bereinigt)", "Online Retail II (bereinigt)"),
  n_zeilen   = c(nrow(rr_time), nrow(or2_time)),
  n_spalten  = c(ncol(rr_time), ncol(or2_time))
)

# Verteilungen kaufbezogener Variablen in Online Retail II
or2_summary <- or2_time %>%
  summarise(
    n_events       = n(),
    n_returns      = sum(event_type == "return", na.rm = TRUE),
    median_qty     = median(quantity_w, na.rm = TRUE),
    p99_qty        = quantile(quantity_w, 0.99, na.rm = TRUE),
    median_price   = median(price_w, na.rm = TRUE),
    p99_price      = quantile(price_w, 0.99, na.rm = TRUE),
    median_revenue = median(revenue_w, na.rm = TRUE),
    p99_revenue    = quantile(revenue_w, 0.99, na.rm = TRUE)
  )

# Stichprobenzeilen zur Illustration
rr_probe  <- head(select(rr_time, customer_id, item_id, timestamp, event_type), 5)
or2_probe <- head(select(or2_time, customer_id, item_id, timestamp, event_type,
                         quantity_w, price_w, revenue_w), 5)

dim_summary
or2_summary
rr_probe
or2_probe
```

Die Übersicht zeigt, dass beide Quellen nach der Bereinigung eine substanzielle Anzahl an Beobachtungen enthalten und somit ausreichend groß für die geplante Clusteranalyse sind.  

Die kaufbezogenen Variablen im Online-Retail-II-Datensatz weisen nach Winsorisierung plausible Wertebereiche auf, extreme Ausreißer wurden erfolgreich begrenzt. Rückgaben sind als eigene Ereignisse gekennzeichnet und können später gesondert berücksichtigt werden.  

Der RetailRocket-Datensatz bildet dagegen ausschließlich Interaktionen ab, liefert aber mit den normalisierten Zeitfeldern eine konsistente Ergänzung zur transaktionsorientierten Sicht von Online Retail II.  

Damit ist eine robuste und vergleichbare Datenbasis geschaffen, auf der in Abschnitt [3.2 Merkmalsbildung](#sec-merkmalset) die eigentlichen Analysemerkmale konstruiert werden.






<!-- ================= Ende ================= -->


